---
title: "Pseudocode Thinking"
author: "Aline Talhouk"
date: "May 2, 2017"
output: html_document
---

Pseudocode to run on cluster (priority) we can work on serial version later. Please note this is not code that runs!

1- BootTrain: Function to train and internally validate algorithm
```{ eval=FALSE}
A function to train model(s) on a bootstrap sample obtained from the data, and to evaluate the model on the out of bag samples not used for training.

Input: 
  data: training data X (n x p)
  class:training labels Y(n x 1)
  seed: the data seed
  algorithm: a vector of algorithms of size a to run (1 x a)

Output:
  model: the model object that was run on the bootstrap data (a list item)
  eval: a data frame with the out of bag evaluation criteria of the model parameters, a line by algorithm



trainBoot <- function(data, class, n, seed = 1, algorithms=NULL) {
  
  # Generate bootstrap resamples; test samples are those not chosen in training
  set.seed(seed)
  class <- as.factor(class)  # ensure class is a factor
  train.idx <- training_id(data = data, class = class, n = n)
  test.idx <- purrr::map(train.idx, ~ which(!seq_len(nrow(data)) %in% .x))
  
  # Classification algorithms to use and their model function calls
  algs <- algorithms %||% ALG.NAME %>%
    stats::setNames(., .)  # if null, use all
  
  # Apply training sets to models and predict on the test sets
  name <- measure <- value <- NULL
  models <- purrr::map(algs,
                       ~ purrr::map(train.idx, function(id) 
                         classification(data[id, ], class[id], .x)))
  preds <- purrr::map(models,
                      ~ purrr::pmap(list(.x, test.idx, train.idx),
                                    prediction, data = data, class = class))
  evals <- purrr::map_at(preds, "pam", purrr::map, 1) %>% 
    purrr::map(~ purrr::map2(test.idx, .x, ~ evaluation(class[.x], .y)) %>%
                 purrr::map_df(purrr::flatten)) %>% 
    tibble::enframe() %>% 
    tidyr::unnest() 
  # May not need to return preds
  return(list(model = models, pred = preds,eval = evals))
}

```

Running 1 on the cluster requires code for the assembly of the eval array for each bootstrap sample


2- ensemble: Function to construct the ensemble 

```{ eval=FALSE}
A function to compute an ensemble of classifiers by ranking and choosing the best performing algorithm from each bootstrap training set and averaging prediction of a new case based on a vote of all the best bootstrap model

Input: 
  a BootTrain object or a list of best algorithms
Output:
  bests: a list of best algorithms for each bootstrap #this is important to avoid recomputing the best each time
  a list of prediction from each bootstrap best #this is not perhaps necessary 
  a final prediction based on majority vote
  
for each slice of the array
# rank the algorithms based on RanAggreg output
  RankAggreg::RankAggreg the evaluation criteria
# Choose the highest ranking algorithms and save the output  
  bests$i <- Select and save the best algorithm
end

preds <- purrr::map(bests,
                      ~ purrr::pmap(list(.x, test.idx, train.idx),
                                    prediction, data = data, class = class)) 
Final pred <- majority vote (preds)

return (bests, preds, Final pred)

```

Select the best 3 algorithms based on all the bootstrap samples and refit them over all the data to be used in predicting new cases

3- Top3 algorithms overall
```{r eval=FALSE}
input:
    data: training data X (n x p)
    class:training labels Y(n x 1)
    bests: a list of best algorithms for each bootstrap 
output: 
    top3models: a list of the most performant models from bootstrap samples, refit       on the whole data

```


