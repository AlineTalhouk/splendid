<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Ensemble Classification using `splendid` • splendid</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Ensemble Classification using `splendid`">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">splendid</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.4.2</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/ensemble_classification.html">Ensemble Classification using `splendid`</a></li>
    <li><a class="dropdown-item" href="../articles/model_extensions.html">Model Extensions</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/AlineTalhouk/splendid/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Ensemble Classification using `splendid`</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/AlineTalhouk/splendid/blob/master/vignettes/ensemble_classification.Rmd" class="external-link"><code>vignettes/ensemble_classification.Rmd</code></a></small>
      <div class="d-none name"><code>ensemble_classification.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>Supervised learning is the branch of statistical learning where we
construct a predictive model with the goal of regression or
classification. The <code>splendid</code> package focuses on
classification with an ensemble framework: many classification
algorithms are used, and prediction is done across bootstrap replicates
of the data. An ensemble classifier is built from the best performing
algorithms according to evaluation measures. No single classifier always
performs the best for every data set that exists, so there is increasing
utility to come up with ensemble classifiers. The objective is to use
this classifier to obtain highly accurate predictions in independent
data sets for the purposes of diagnostic identification. In genomic
studies for example, one might be interested in using the class labels
of a cancer subtype in one cohort to predict the subtypes in an
independent cohort.</p>
<p><code>splendid</code> is currently only available on GitHub (<em>the
second line below will be uncommented once the repository becomes
public</em>).</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># install.packages("devtools")</span></span>
<span><span class="co"># devtools::install_github("AlineTalhouk/splendid")</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/AlineTalhouk/splendid" class="external-link">splendid</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://yihui.org/knitr/" class="external-link">knitr</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">hgsc</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="overview">Overview<a class="anchor" aria-label="anchor" href="#overview"></a>
</h2>
<p>The main function of <code>splendid</code> is
<code><a href="../reference/splendid.html">splendid()</a></code>, and the usage is intuitive:</p>
<ul>
<li>
<code>data</code>: your rectangular data in standard form (rows as
samples/observations, columns as variables/features)</li>
<li>
<code>class</code>: the true/reference class labels with length
equal to the number of samples</li>
<li>
<code>algorithms</code>: character vector of classification
algorithms</li>
<li>
<code>n</code>: number of bootstrap replicates to generate as
training sets. Test sets are taken from the out-of-bag samples
(OOB).</li>
<li>
<code>seed_boot</code>: seed for reproducibility in bootstrapping
training sets for model generation</li>
<li>
<code>seed_samp</code>: seed for reproducibility in subsampling
training sets for model generation</li>
<li>
<code>seed_alg</code>: seed for reproducibility for algorithms with
a random training aspect (e.g. random forests)</li>
<li>
<code>convert</code>: whether to convert categorical predictors to
dummy variables</li>
<li>
<code>rfe</code>: whether to use Recursive Feature Elimination (RFE)
for feature selection. Only implemented for certain algorithms.</li>
<li>
<code>ova</code>: whether to run one-vs-all versions for each
algorithm (e.g. class 1 vs. not class 1, class 2 vs. not class 2,
etc.)</li>
<li>
<code>standardize</code>: whether to standardize the training set
before classification and the test set (using training set centerings
&amp; scalings) during prediction</li>
<li>
<code>sampling</code>: the subsampling method is either up-sampling,
down-sampling, or SMOTE (hybrid method)</li>
<li>
<code>stratify</code>: whether to use stratified bootstrap sampling
to ensure that the proportion of classes in the bootstrap sample is the
same as the original data</li>
<li>
<code>plus</code>: whether to calculate the .632+ or .632 estimator
for the log loss error rate</li>
<li>
<code>threshold</code>: predicted probabilities below this value are
classified as “unclassified” instead of imposing one of the labels from
<code>class</code>
</li>
<li>
<code>trees</code>: number of trees/iterations to use in random
forest/boosting models, respectively</li>
<li>
<code>tune</code>: whether to tune hyperparameters in pertinent
algorithms</li>
<li>
<code>top</code>: how many of the top performing algorithms are
chosen for the final ensemble?</li>
<li>
<code>seed_rank</code>: random seed used for reproducibility in rank
aggregation of ensemble algorithms</li>
<li>
<code>sequential</code>: whether to run the sequential ensemble</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">hgsc</span>, <span class="st">"class.true"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt; class</span></span>
<span><span class="co">#&gt; DIF.C4 IMM.C2 MES.C1 PRO.C5 </span></span>
<span><span class="co">#&gt;    135    107    109    138</span></span>
<span><span class="va">sl_result</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/splendid.html">splendid</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">hgsc</span>, class <span class="op">=</span> <span class="va">class</span>, n <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                      algorithms <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"slda"</span>, <span class="st">"knn"</span>, <span class="st">"svm"</span><span class="op">)</span>, rfe <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p>The resulting object is a list with the following elements:</p>
<ul>
<li>
<code>models</code> gives the model fits for each of the algorithms
used. The fits are themselves nested based on the number of bootstrap
replicates.</li>
<li>
<code>preds</code> has the same hierarchical structure as
<code>models</code>, except the metadata consists of the predicted class
labels.</li>
<li>
<code>evals</code> is a list of data frames showing evaluation
metrics for each algorithm. Rows are metrics and columns are bootstrap
replicates. More details on this element will be provided later.</li>
<li>
<code>bests</code> is a vector that shows the best-performing
algorithm for each bootstrapped replicate of the data, chosen by rank
aggregation.</li>
<li>
<code>ensemble_algs</code> tallies the algorithm frequencies in
<code>bests</code>, returning the top (3, by default) algorithms
chosen.</li>
<li>
<code>ensemble</code> is a list of model fits for each of the
algorithms in <code>ensemble_algs</code>, fit on the full data.</li>
</ul>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">sl_result</span>, max.level <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 8</span></span>
<span><span class="co">#&gt;  $ models       :List of 3</span></span>
<span><span class="co">#&gt;   ..$ slda:List of 2</span></span>
<span><span class="co">#&gt;   ..$ knn :List of 2</span></span>
<span><span class="co">#&gt;   ..$ svm :List of 2</span></span>
<span><span class="co">#&gt;  $ preds        :List of 3</span></span>
<span><span class="co">#&gt;   ..$ slda:List of 2</span></span>
<span><span class="co">#&gt;   ..$ knn :List of 2</span></span>
<span><span class="co">#&gt;   ..$ svm :List of 2</span></span>
<span><span class="co">#&gt;  $ evals        :List of 3</span></span>
<span><span class="co">#&gt;   ..$ slda:'data.frame': 48 obs. of  2 variables:</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "err_632plus")= num 1.89</span></span>
<span><span class="co">#&gt;   ..$ knn :'data.frame': 48 obs. of  2 variables:</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "err_632plus")= num 1.42</span></span>
<span><span class="co">#&gt;   ..$ svm :'data.frame': 48 obs. of  2 variables:</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "err_632plus")= num 0.209</span></span>
<span><span class="co">#&gt;  $ bests        : Named chr [1:2] "svm" "svm"</span></span>
<span><span class="co">#&gt;   ..- attr(*, "names")= chr [1:2] "1" "2"</span></span>
<span><span class="co">#&gt;  $ ensemble_algs: chr "svm"</span></span>
<span><span class="co">#&gt;  $ ensemble_mods:List of 1</span></span>
<span><span class="co">#&gt;   ..$ :List of 30</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class")= chr "svm"</span></span>
<span><span class="co">#&gt;  $ seq_mods     : NULL</span></span>
<span><span class="co">#&gt;  $ seq_preds    : NULL</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="classification">Classification<a class="anchor" aria-label="anchor" href="#classification"></a>
</h2>
<p>The first step in the <code>splendid</code> pipeline is
classification. Given explanatory data and a reference response of
classes, we wish to build a classifier that can accurately predict class
representation in a separate validation data set. To avoid overfitting,
we split the given data into a training set and test set. The training
set is constructed by taking a random sample with replacement of all
samples. The test set is comprised of all samples which are not included
in the corresponding training set, also known as an OOB sample. This is
a simple bootstrap resampling scheme, which we replicate a sufficient
number of times to capture sampling variability.</p>
<div class="section level3">
<h3 id="algorithms">Algorithms<a class="anchor" aria-label="anchor" href="#algorithms"></a>
</h3>
<p>There exist a vast number of classification algorithms. Those
currently in <code>splendid</code> are:</p>
<ul>
<li>Prediction Analysis for Microarrays (<code>"pam"</code>)</li>
<li>Support Vector Machines using a Radial-Basis
Kernel(<code>"svm"</code>)</li>
<li>Random Forests (<code>"rf"</code>)</li>
<li>Linear Discriminant Analysis (<code>"lda"</code>)</li>
<li>Shrinkage Linear Discriminant Analysis
(<code>"slda"</code>)<sup>1</sup>
</li>
<li>Shrinkage Diagonal Discriminant Analysis
(<code>"sdda"</code>)<sup>1</sup>
</li>
<li>Multinomial Logistic Regression using
<ul>
<li>Generalized Linear Model with no penalization
(<code>"mlr_glm"</code>)<sup>2</sup>
</li>
<li>GLM with LASSO penalty (<code>"mlr_lasso"</code>)<sup>2</sup>
</li>
<li>GLM with ridge penalty (<code>"mlr_ridge"</code>)<sup>2</sup>
</li>
<li>Neural Networks (<code>"mlr_nnet"</code>)</li>
</ul>
</li>
<li>Neural Networks with One Hidden Layer (<code>"nnet"</code>)</li>
<li>Naive Bayes (<code>"nbayes"</code>)</li>
<li>AdaBoost.m1 (<code>"adaboost_m1"</code>)</li>
<li>Extreme Gradient Boosting using Decision Trees
(<code>"xgboost"</code>)</li>
<li>K-Nearest Neighbours (<code>"knn"</code>)</li>
</ul>
<p>These algorithms are implemented in
<code><a href="../reference/classification.html">classification()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="hyperparameters-and-other-details">Hyperparameters and other Details<a class="anchor" aria-label="anchor" href="#hyperparameters-and-other-details"></a>
</h3>
<p>Certain functions have hyperparameters that need to be tuned in order
to select the best model before prediction. We use a grid search on a
pre-specified range of the hyperparameters and choose the optimal values
using <code><a href="https://rdrr.io/pkg/caret/man/train.html" class="external-link">caret::train()</a></code>.</p>
<p>The ranges for the tuning parameters are:</p>
<ul>
<li>
<code>svm</code>
<ul>
<li>
<code>sigma</code>: <code>1 / ncol(data) * 2 ^ (0:4)</code>
</li>
<li>
<code>C</code>: 1, 2, 4, 8, 16</li>
</ul>
</li>
<li>
<code>rf</code>
<ul>
<li>
<code>mtry</code>: 1, 4, 9, 16, 25</li>
</ul>
</li>
<li>
<code>adaboost_m1</code>
<ul>
<li>
<code>mfinal</code>: 1, 2, 3, 4, 5</li>
<li>
<code>maxdepth</code>: 1, 2, 3, 4, 5</li>
<li>
<code>coeflearn</code>: “Breiman”, “Freund”, “Zhu”</li>
</ul>
</li>
</ul>
<p>We use <code><a href="https://rdrr.io/pkg/e1071/man/tune.html" class="external-link">e1071::tune()</a></code> for neural network parameters. The
ranges are:</p>
<ul>
<li>
<code>nnet</code>
<ul>
<li>
<code>size</code>: 1, 2, 3, 4, 5</li>
<li>
<code>decay</code>: 0, 0.125, 0.25, 0.375, 0.5</li>
</ul>
</li>
</ul>
<p>Some algorithms have certain properties that require data
manipulations before classification.</p>
<ul>
<li>
<code>slda</code> and <code>sdda</code>: These shrinkage
discriminant analysis algorithms estimate the regularization parameters
analytically without having to solve using resampling methods.</li>
<li>
<code>multinom_nnet</code> and <code>nnet</code>: We increase the
maximum number of allowable weights to a sufficiently large number to
avoid ill-fitted models.</li>
<li>
<code>knn</code>: Since K-Nearest Neighbours is a non-parametric
method, there <em>is</em> no model object upon which predictions are
made, and predicted classes are directly found. To obtain prediction
probabilities by class, we use a distance matrix between the training
and test set samples.</li>
<li>
<code>pam</code>: a uniform prior is used in the training step.</li>
<li>
<code>xgboost</code>: the objective function is soft probability,
and the evaluation metric is log loss.</li>
</ul>
</div>
<div class="section level3">
<h3 id="feature-selection">Feature Selection<a class="anchor" aria-label="anchor" href="#feature-selection"></a>
</h3>
<p>We use Recursive Feature Elimination (RFE)<sup>3</sup> on
<code>lda</code>, <code>rf</code>, <code>svm</code>, and
<code>adaboost_m1</code> to reduce the dimensionality before
classification. In <code>svm</code>, we do this before tuning because of
the computational complexity. However, we can embed this feature
selection within the tuning step for the other three algorithms. Set
<code>rfe = TRUE</code> in <code>splendid</code> to use feature
selection.</p>
<p>One limitation of RFE is that an a priori set of feature subset sizes
need to be specified, determining the search space for the algorithm. By
default, we set the <code>sizes</code> parameter to be every 5 integers
from 0 up to one-half of the smallest class size. Recall the class
sizes:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/table.html" class="external-link">table</a></span><span class="op">(</span><span class="va">class</span><span class="op">)</span></span>
<span><span class="co">#&gt; class</span></span>
<span><span class="co">#&gt; DIF.C4 IMM.C2 MES.C1 PRO.C5 </span></span>
<span><span class="co">#&gt;    135    107    109    138</span></span></code></pre></div>
<p>We tell RFE to search for the best models with 5, 10, …, 50 features.
Cross-validation with 2 folds is used in the algorithm.</p>
</div>
</div>
<div class="section level2">
<h2 id="prediction">Prediction<a class="anchor" aria-label="anchor" href="#prediction"></a>
</h2>
<p>There is a different prediction method for each classifier, since
they are all imported from an external package. The
<code><a href="../reference/prediction.html">prediction()</a></code> function calls each method based on the class
of the model output from <code><a href="../reference/classification.html">classification()</a></code>.
<code><a href="../reference/prediction.html">prediction()</a></code> also performs some manipulations so that the
results all have the same data structure: unnamed factors, with labels
given in the same order as the true class labels.</p>
<p>The method for <code>"pam"</code> is an exception: the output from
<code><a href="../reference/prediction.html">prediction()</a></code> not only has the predicted class labels, but
also the cross-validated threshold value calculated from the training
set to use in prediction on the test set, named <code>delta</code>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">sl_result</span><span class="op">$</span><span class="va">preds</span>, max.level <span class="op">=</span> <span class="fl">2</span>, list.len <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="co">#&gt; List of 3</span></span>
<span><span class="co">#&gt;  $ slda:List of 2</span></span>
<span><span class="co">#&gt;   ..$ : Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 2 1 1 1 1 1 1 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "prob")= num [1:178, 1:4] 0 0 0.0037 1 1 ...</span></span>
<span><span class="co">#&gt;   .. .. ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.true")= Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 2 1 1 1 4 1 2 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.thres")= Factor w/ 5 levels "DIF.C4","IMM.C2",..: 4 3 2 1 1 1 1 1 1 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.prop")= num 1</span></span>
<span><span class="co">#&gt;   ..$ : Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 1 2 4 1 1 1 1 1 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "prob")= num [1:161, 1:4] 0.002975 0.000202 0.999009 0.236365 0 ...</span></span>
<span><span class="co">#&gt;   .. .. ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.true")= Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 4 2 4 4 1 1 1 1 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.thres")= Factor w/ 5 levels "DIF.C4","IMM.C2",..: 4 3 1 2 4 1 1 1 1 1 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.prop")= num 1</span></span>
<span><span class="co">#&gt;  $ knn :List of 2</span></span>
<span><span class="co">#&gt;   ..$ : Factor w/ 4 levels "DIF.C4","IMM.C2",..: 3 3 2 1 1 4 4 1 2 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "prob")= num [1:178, 1:4] 0.2 0 0 1 1 0.4 0 1 0.4 0 ...</span></span>
<span><span class="co">#&gt;   .. .. ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.true")= Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 2 1 1 1 4 1 2 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.thres")= Factor w/ 5 levels "DIF.C4","IMM.C2",..: 3 3 2 1 1 4 4 1 2 4 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.prop")= num 1</span></span>
<span><span class="co">#&gt;   ..$ : Factor w/ 4 levels "DIF.C4","IMM.C2",..: 1 3 1 2 4 1 1 1 2 2 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "prob")= num [1:161, 1:4] 0.6 0.167 0.667 0 0 ...</span></span>
<span><span class="co">#&gt;   .. .. ..- attr(*, "dimnames")=List of 2</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.true")= Factor w/ 4 levels "DIF.C4","IMM.C2",..: 4 3 4 2 4 4 1 1 1 1 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.thres")= Factor w/ 5 levels "DIF.C4","IMM.C2",..: 1 3 1 2 4 1 1 1 1 2 ...</span></span>
<span><span class="co">#&gt;   .. ..- attr(*, "class.prop")= num 1</span></span>
<span><span class="co">#&gt;   [list output truncated]</span></span></code></pre></div>
<div class="section level3">
<h3 id="attributes">Attributes<a class="anchor" aria-label="anchor" href="#attributes"></a>
</h3>
<p>Note that each predicted class has an attribute that shows the class
probabilities in a matrix. We ensure that the class probabilities for
every sample sum to one by making a small adjustment to one of the
classes depending on whether the probability was over or under one.
These matrices are useful to compute evaluation measures and generate
discriminating graphs that we detail below, and is stored in
<code>attr(*, "prob")</code>.</p>
<p>To obtain better performance under evaluation metrics, we may want to
exclude samples which have a maximum class probability below a
threshold. For example, we may only compare the true test labels with
the corresponding OOB samples where the winning class has a probability
at least 50%. Samples below this threshold are labelled as
<code>"unclassified"</code>. If the threshold results in all samples
being unclassified, then we use the unfiltered, original predicted
labels for evaluation. This object is stored in
<code>attr(*, "class.thres")</code>.</p>
<p>The final attribute of a prediction object is stored
<code>attr(*, "class.prop")</code>, showing the proportion of classified
cases. This value is inversely proportional to the
<code>threshold</code> setting.</p>
</div>
</div>
<div class="section level2">
<h2 id="evaluation">Evaluation<a class="anchor" aria-label="anchor" href="#evaluation"></a>
</h2>
<div class="section level3">
<h3 id="metrics">Metrics<a class="anchor" aria-label="anchor" href="#metrics"></a>
</h3>
<p>Evaluation measures are important because they tell us the prediction
performance of a classifier. The table below shows all the measures for
<code>svm</code>, for two bootstrap replicates. The
<code>logloss</code>, <code>auc</code> and <code>pdi</code> measures
make use of the prediction probabilities, whereas the rest are computed
from the multiclass confusion matrix of reference and predicted class
labels.</p>
<table class="table">
<caption>SVM Evaluation Metrics</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">logloss</td>
<td align="right">0.2947182</td>
<td align="right">0.2540596</td>
</tr>
<tr class="even">
<td align="left">auc</td>
<td align="right">0.9858863</td>
<td align="right">0.9899632</td>
</tr>
<tr class="odd">
<td align="left">pdi</td>
<td align="right">0.9619136</td>
<td align="right">0.9731234</td>
</tr>
<tr class="even">
<td align="left">accuracy</td>
<td align="right">0.8876404</td>
<td align="right">0.9378882</td>
</tr>
<tr class="odd">
<td align="left">macro_ppv</td>
<td align="right">0.8882541</td>
<td align="right">0.9354862</td>
</tr>
<tr class="even">
<td align="left">macro_npv</td>
<td align="right">0.9625091</td>
<td align="right">0.9792366</td>
</tr>
<tr class="odd">
<td align="left">macro_sensitivity</td>
<td align="right">0.8875489</td>
<td align="right">0.9354862</td>
</tr>
<tr class="even">
<td align="left">macro_specificity</td>
<td align="right">0.9624474</td>
<td align="right">0.9792366</td>
</tr>
<tr class="odd">
<td align="left">macro_f1</td>
<td align="right">0.8877453</td>
<td align="right">0.9353769</td>
</tr>
<tr class="even">
<td align="left">mcc</td>
<td align="right">0.8500927</td>
<td align="right">0.9161892</td>
</tr>
<tr class="odd">
<td align="left">kappa</td>
<td align="right">0.8499853</td>
<td align="right">0.9161415</td>
</tr>
<tr class="even">
<td align="left">gmean</td>
<td align="right">0.8871099</td>
<td align="right">0.9353169</td>
</tr>
<tr class="odd">
<td align="left">accuracy.DIF.C4</td>
<td align="right">0.9382022</td>
<td align="right">0.9627329</td>
</tr>
<tr class="even">
<td align="left">accuracy.IMM.C2</td>
<td align="right">0.9382022</td>
<td align="right">0.9689441</td>
</tr>
<tr class="odd">
<td align="left">accuracy.MES.C1</td>
<td align="right">0.9662921</td>
<td align="right">0.9689441</td>
</tr>
<tr class="even">
<td align="left">accuracy.PRO.C5</td>
<td align="right">0.9325843</td>
<td align="right">0.9751553</td>
</tr>
<tr class="odd">
<td align="left">ppv.DIF.C4</td>
<td align="right">0.8936170</td>
<td align="right">0.9361702</td>
</tr>
<tr class="even">
<td align="left">ppv.IMM.C2</td>
<td align="right">0.8750000</td>
<td align="right">0.9090909</td>
</tr>
<tr class="odd">
<td align="left">ppv.MES.C1</td>
<td align="right">0.9302326</td>
<td align="right">0.9375000</td>
</tr>
<tr class="even">
<td align="left">ppv.PRO.C5</td>
<td align="right">0.8541667</td>
<td align="right">0.9591837</td>
</tr>
<tr class="odd">
<td align="left">npv.DIF.C4</td>
<td align="right">0.9541985</td>
<td align="right">0.9736842</td>
</tr>
<tr class="even">
<td align="left">npv.IMM.C2</td>
<td align="right">0.9565217</td>
<td align="right">0.9843750</td>
</tr>
<tr class="odd">
<td align="left">npv.MES.C1</td>
<td align="right">0.9777778</td>
<td align="right">0.9767442</td>
</tr>
<tr class="even">
<td align="left">npv.PRO.C5</td>
<td align="right">0.9615385</td>
<td align="right">0.9821429</td>
</tr>
<tr class="odd">
<td align="left">sensitivity.DIF.C4</td>
<td align="right">0.8750000</td>
<td align="right">0.9361702</td>
</tr>
<tr class="even">
<td align="left">sensitivity.IMM.C2</td>
<td align="right">0.8536585</td>
<td align="right">0.9375000</td>
</tr>
<tr class="odd">
<td align="left">sensitivity.MES.C1</td>
<td align="right">0.9302326</td>
<td align="right">0.9090909</td>
</tr>
<tr class="even">
<td align="left">sensitivity.PRO.C5</td>
<td align="right">0.8913043</td>
<td align="right">0.9591837</td>
</tr>
<tr class="odd">
<td align="left">specificity.DIF.C4</td>
<td align="right">0.9615385</td>
<td align="right">0.9736842</td>
</tr>
<tr class="even">
<td align="left">specificity.IMM.C2</td>
<td align="right">0.9635036</td>
<td align="right">0.9767442</td>
</tr>
<tr class="odd">
<td align="left">specificity.MES.C1</td>
<td align="right">0.9777778</td>
<td align="right">0.9843750</td>
</tr>
<tr class="even">
<td align="left">specificity.PRO.C5</td>
<td align="right">0.9469697</td>
<td align="right">0.9821429</td>
</tr>
<tr class="odd">
<td align="left">f1.DIF.C4</td>
<td align="right">0.8842105</td>
<td align="right">0.9361702</td>
</tr>
<tr class="even">
<td align="left">f1.IMM.C2</td>
<td align="right">0.8641975</td>
<td align="right">0.9230769</td>
</tr>
<tr class="odd">
<td align="left">f1.MES.C1</td>
<td align="right">0.9302326</td>
<td align="right">0.9230769</td>
</tr>
<tr class="even">
<td align="left">f1.PRO.C5</td>
<td align="right">0.8723404</td>
<td align="right">0.9591837</td>
</tr>
<tr class="odd">
<td align="left">mcc.DIF.C4</td>
<td align="right">0.8421581</td>
<td align="right">0.9098544</td>
</tr>
<tr class="even">
<td align="left">mcc.IMM.C2</td>
<td align="right">0.8243107</td>
<td align="right">0.9037953</td>
</tr>
<tr class="odd">
<td align="left">mcc.MES.C1</td>
<td align="right">0.9080103</td>
<td align="right">0.9037953</td>
</tr>
<tr class="even">
<td align="left">mcc.PRO.C5</td>
<td align="right">0.8269126</td>
<td align="right">0.9413265</td>
</tr>
<tr class="odd">
<td align="left">kappa.DIF.C4</td>
<td align="right">0.8420713</td>
<td align="right">0.9098544</td>
</tr>
<tr class="even">
<td align="left">kappa.IMM.C2</td>
<td align="right">0.8242054</td>
<td align="right">0.9036274</td>
</tr>
<tr class="odd">
<td align="left">kappa.MES.C1</td>
<td align="right">0.9080103</td>
<td align="right">0.9036274</td>
</tr>
<tr class="even">
<td align="left">kappa.PRO.C5</td>
<td align="right">0.8265671</td>
<td align="right">0.9413265</td>
</tr>
<tr class="odd">
<td align="left">gmean.DIF.C4</td>
<td align="right">0.9172492</td>
<td align="right">0.9547430</td>
</tr>
<tr class="even">
<td align="left">gmean.IMM.C2</td>
<td align="right">0.9069196</td>
<td align="right">0.9569209</td>
</tr>
<tr class="odd">
<td align="left">gmean.MES.C1</td>
<td align="right">0.9537089</td>
<td align="right">0.9459843</td>
</tr>
<tr class="even">
<td align="left">gmean.PRO.C5</td>
<td align="right">0.9187155</td>
<td align="right">0.9705954</td>
</tr>
</tbody>
</table>
<ul>
<li>
<code>logloss</code>: Multiclass log loss should be minimized</li>
<li>
<code>auc</code>: Area Under the Curve compares trade-off between
sensitivity and specificity</li>
<li>
<code>pdi</code>: Polytomous Discrimination Index</li>
<li>
<code>accuracy</code>: number of correctly classified samples out of
all samples</li>
<li>
<code>ppv</code>: positive predictive value, also known as
precision</li>
<li>
<code>npv</code>: negative predictive value</li>
<li>
<code>sensitivity</code>: also known as recall</li>
<li>
<code>specificity</code>: also known as selectivity</li>
<li>
<code>f1</code>: balanced metric between precision and recall</li>
<li>
<code>mcc</code>: Matthew’s correlation coefficient</li>
</ul>
<p>Note that in the multiclass case, we have variants for some of the
measures listed above, depending on how we are looking at the data. A
<code>macro</code>-averaged metric calculates said metric on each of the
one-vs-all confusion matrices and then takes the mean. A
<code>micro</code>-averaged metric is calculated on the element-wise sum
of all one-vs-all confusion matrices.</p>
<p>Hence, there are <code>macro_ppv</code>, <code>macro_npv</code>,
<code>macro_sensitivity</code>, <code>macro_specificity</code>, and
<code>macro_f1</code> macro-averaged metrics, and <code>micro_mcc</code>
as the “only” micro-averaged metric. It turns out that
<code>accuracy</code> is mathematically equivalent to micro-averaged
PPV, sensitivity, and F1-score, so we don’t redundantly add those to the
list. Furthermore, calculating <code>mcc</code> in a macro-averaged way
is not recommended, so we use it on the full confusion matrix.</p>
<p>Class-specific measures fill in the rest of the matrix, labelled with
the naming scheme <code>metric.class</code>.</p>
<p>The .632 estimator are implemented for the multiclass log loss
function. This error estimate aims to make a compromise between an
overbiased prediction like the leave-one-out bootstrap error with an
underbiased prediction like the training (or apparent) error. If we set
<code>plus = TRUE</code> in <code><a href="../reference/splendid.html">splendid()</a></code> (default), one can
calculate the .632+ estimator, an improvement over the .632 estimator
that takes into account the amount of overfitting.<sup>5</sup> The value
is stored as an attribute in the evaluation object:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">vapply</a></span><span class="op">(</span><span class="va">sl_result</span><span class="op">$</span><span class="va">evals</span>, <span class="va">attr</span>, <span class="st">"err_632plus"</span>, FUN.VALUE <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/double.html" class="external-link">double</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt;      slda       knn       svm </span></span>
<span><span class="co">#&gt; 1.8921645 1.4202706 0.2089945</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="plots">Plots<a class="anchor" aria-label="anchor" href="#plots"></a>
</h3>
<p>To assess the performance of a classifier, we can look at a
discriminating plot and reliability plot.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">hgsc</span><span class="op">)</span></span>
<span><span class="va">class</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">hgsc</span>, <span class="st">"class.true"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="va">training.id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">class</span><span class="op">)</span>, replace <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="va">test.id</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/which.html" class="external-link">which</a></span><span class="op">(</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq_along</a></span><span class="op">(</span><span class="va">class</span><span class="op">)</span> <span class="op"><a href="https://rdrr.io/r/base/match.html" class="external-link">%in%</a></span> <span class="va">training.id</span><span class="op">)</span></span>
<span><span class="va">mod</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/classification.html">classification</a></span><span class="op">(</span><span class="va">hgsc</span><span class="op">[</span><span class="va">training.id</span>, <span class="op">]</span>, <span class="va">class</span><span class="op">[</span><span class="va">training.id</span><span class="op">]</span>, <span class="st">"rf"</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/prediction.html">prediction</a></span><span class="op">(</span><span class="va">mod</span>, <span class="va">hgsc</span>, <span class="va">test.id</span>, class <span class="op">=</span> <span class="va">class</span><span class="op">)</span></span>
<span><span class="va">evals</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/evaluation.html">evaluation</a></span><span class="op">(</span><span class="va">class</span><span class="op">[</span><span class="va">test.id</span><span class="op">]</span>, <span class="va">pred</span>, plot <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<p><img src="ensemble_classification_files/figure-html/plots-1.png" width="576"><img src="ensemble_classification_files/figure-html/plots-2.png" width="576"><img src="ensemble_classification_files/figure-html/plots-3.png" width="576"></p>
</div>
</div>
<div class="section level2">
<h2 id="ensemble-construction">Ensemble Construction<a class="anchor" aria-label="anchor" href="#ensemble-construction"></a>
</h2>
<p>For each bootstrap replicate training set, we want to find the top
performing classifier. We use Rank Aggregation with the Genetic
Algorithm to choose the top algorithm by comparing across evaluation
metrics<sup>4</sup>. In the case of log loss, we need to first invert
its value since it its objective function is minimization.
Class-specific measures are not included in the rank aggregation because
they are interdependent. For example, a sample with a true class of “A”
should not be treated differently depending on whether it was
misclassified into “B” or “C”.</p>
<p>After obtaining the list of top classifiers for each bootstrap
replicate, we sort them by decreasing frequency and keep the top 3 to
use in the ensemble. The <code>ensemble_mods</code> output of
<code>splendid</code> is a list of models fit on the full data for the
top classifiers.</p>
</div>
<div class="section level2">
<h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<ol style="list-style-type: decimal">
<li>Ahdesmäki, Miika, and Korbinian Strimmer. “Feature selection in
omics prediction problems using cat scores and false nondiscovery rate
control.” <em>The Annals of Applied Statistics</em> 4.1 (2010):
503-519.</li>
<li><a href="https://web.stanford.edu/~hastie/glmnet/glmnet_beta.html" class="external-link uri">https://web.stanford.edu/~hastie/glmnet/glmnet_beta.html</a></li>
<li><a href="https://topepo.github.io/caret/recursive-feature-elimination.html" class="external-link uri">https://topepo.github.io/caret/recursive-feature-elimination.html</a></li>
<li>Pihur, Vasyl, Susmita Datta, and Somnath Datta. “RankAggreg, an R
package for weighted rank aggregation.” <em>BMC bioinformatics</em> 10.1
(2009): 62.</li>
<li>Efron, Bradley and Tibshirani, Robert (1997), “Improvements on
Cross-Validation: The .632+ Bootstrap Method,” Journal of American
Statistical Association, 92, 438, 548-560.</li>
</ol>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Derek Chiu, Aline Talhouk, Dustin Johnson.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
