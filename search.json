[{"path":"https://alinetalhouk.github.io/splendid/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2021 splendid authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Ensemble Classification using `splendid`","text":"Supervised learning branch statistical learning construct predictive model goal regression classification. splendid package focuses classification ensemble framework: many classification algorithms used, prediction done across bootstrap replicates data. ensemble classifier built best performing algorithms according evaluation measures. single classifier always performs best every data set exists, increasing utility come ensemble classifiers. objective use classifier obtain highly accurate predictions independent data sets purposes diagnostic identification. genomic studies example, one might interested using class labels cancer subtype one cohort predict subtypes independent cohort. splendid currently available GitHub (second line uncommented repository becomes public).","code":"# install.packages(\"devtools\") # devtools::install_github(\"AlineTalhouk/splendid\") library(splendid) library(knitr) data(hgsc)"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"Ensemble Classification using `splendid`","text":"main function splendid splendid(), usage intuitive: data: rectangular data standard form (rows samples/observations, columns variables/features) class: true/reference class labels length equal number samples algorithms: character vector classification algorithms n: number bootstrap replicates generate training sets. Test sets taken --bag samples (OOB). seed_boot: seed reproducibility bootstrapping training sets model generation seed_samp: seed reproducibility subsampling training sets model generation seed_alg: seed reproducibility algorithms random training aspect (e.g. random forests) convert: whether convert categorical predictors dummy variables rfe: whether use Recursive Feature Elimination (RFE) feature selection. implemented certain algorithms. ova: whether run one-vs-versions algorithm (e.g. class 1 vs. class 1, class 2 vs. class 2, etc.) standardize: whether standardize training set classification test set (using training set centerings & scalings) prediction sampling: subsampling method either -sampling, -sampling, SMOTE (hybrid method) stratify: whether use stratified bootstrap sampling ensure proportion classes bootstrap sample original data plus: whether calculate .632+ .632 estimator log loss error rate threshold: predicted probabilities value classified “unclassified” instead imposing one labels class trees: number trees/iterations use random forest/boosting models, respectively tune: whether tune hyperparameters pertinent algorithms top: many top performing algorithms chosen final ensemble? seed_rank: random seed used reproducibility rank aggregation ensemble algorithms sequential: whether run sequential ensemble resulting object list following elements: models gives model fits algorithms used. fits nested based number bootstrap replicates. preds hierarchical structure models, except metadata consists predicted class labels. evals list data frames showing evaluation metrics algorithm. Rows metrics columns bootstrap replicates. details element provided later. bests vector shows best-performing algorithm bootstrapped replicate data, chosen rank aggregation. ensemble_algs tallies algorithm frequencies bests, returning top (3, default) algorithms chosen. ensemble list model fits algorithms ensemble_algs, fit full data.","code":"class <- attr(hgsc, \"class.true\") table(class) #> class #> DIF.C4 IMM.C2 MES.C1 PRO.C5  #>    135    107    109    138 sl_result <- splendid(data = hgsc, class = class, n = 2,                       algorithms = c(\"slda\", \"knn\", \"svm\"), rfe = FALSE) str(sl_result, max.level = 2) #> List of 8 #>  $ models       :List of 3 #>   ..$ slda:List of 2 #>   ..$ knn :List of 2 #>   ..$ svm :List of 2 #>  $ preds        :List of 3 #>   ..$ slda:List of 2 #>   ..$ knn :List of 2 #>   ..$ svm :List of 2 #>  $ evals        :List of 3 #>   ..$ slda:'data.frame': 48 obs. of  2 variables: #>   .. ..- attr(*, \"err_632plus\")= num 1.89 #>   ..$ knn :'data.frame': 48 obs. of  2 variables: #>   .. ..- attr(*, \"err_632plus\")= num 1.42 #>   ..$ svm :'data.frame': 48 obs. of  2 variables: #>   .. ..- attr(*, \"err_632plus\")= num 0.209 #>  $ bests        : Named chr [1:2] \"svm\" \"svm\" #>   ..- attr(*, \"names\")= chr [1:2] \"1\" \"2\" #>  $ ensemble_algs: chr \"svm\" #>  $ ensemble_mods:List of 1 #>   ..$ :List of 30 #>   .. ..- attr(*, \"class\")= chr \"svm\" #>  $ seq_mods     : NULL #>  $ seq_preds    : NULL"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"classification","dir":"Articles","previous_headings":"","what":"Classification","title":"Ensemble Classification using `splendid`","text":"first step splendid pipeline classification. Given explanatory data reference response classes, wish build classifier can accurately predict class representation separate validation data set. avoid overfitting, split given data training set test set. training set constructed taking random sample replacement samples. test set comprised samples included corresponding training set, also known OOB sample. simple bootstrap resampling scheme, replicate sufficient number times capture sampling variability.","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"algorithms","dir":"Articles","previous_headings":"Classification","what":"Algorithms","title":"Ensemble Classification using `splendid`","text":"exist vast number classification algorithms. currently splendid : Prediction Analysis Microarrays (\"pam\") Support Vector Machines using Radial-Basis Kernel(\"svm\") Random Forests (\"rf\") Linear Discriminant Analysis (\"lda\") Shrinkage Linear Discriminant Analysis (\"slda\")1 Shrinkage Diagonal Discriminant Analysis (\"sdda\")1 Generalized Linear Model penalization (\"mlr_glm\")2 GLM LASSO penalty (\"mlr_lasso\")2 GLM ridge penalty (\"mlr_ridge\")2 Neural Networks (\"mlr_nnet\") Neural Networks One Hidden Layer (\"nnet\") Naive Bayes (\"nbayes\") AdaBoost.m1 (\"adaboost_m1\") Extreme Gradient Boosting using Decision Trees (\"xgboost\") K-Nearest Neighbours (\"knn\") algorithms implemented classification().","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"hyperparameters-and-other-details","dir":"Articles","previous_headings":"Classification","what":"Hyperparameters and other Details","title":"Ensemble Classification using `splendid`","text":"Certain functions hyperparameters need tuned order select best model prediction. use grid search pre-specified range hyperparameters choose optimal values using caret::train(). ranges tuning parameters : sigma: 1 / ncol(data) * 2 ^ (0:4) C: 1, 2, 4, 8, 16 mtry: 1, 4, 9, 16, 25 mfinal: 1, 2, 3, 4, 5 maxdepth: 1, 2, 3, 4, 5 coeflearn: “Breiman”, “Freund”, “Zhu” use e1071::tune() neural network parameters. ranges : size: 1, 2, 3, 4, 5 decay: 0, 0.125, 0.25, 0.375, 0.5 algorithms certain properties require data manipulations classification. slda sdda: shrinkage discriminant analysis algorithms estimate regularization parameters analytically without solve using resampling methods. multinom_nnet nnet: increase maximum number allowable weights sufficiently large number avoid ill-fitted models. knn: Since K-Nearest Neighbours non-parametric method, model object upon predictions made, predicted classes directly found. obtain prediction probabilities class, use distance matrix training test set samples. pam: uniform prior used training step. xgboost: objective function soft probability, evaluation metric log loss.","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"feature-selection","dir":"Articles","previous_headings":"Classification","what":"Feature Selection","title":"Ensemble Classification using `splendid`","text":"use Recursive Feature Elimination (RFE)3 lda, rf, svm, adaboost_m1 reduce dimensionality classification. svm, tuning computational complexity. However, can embed feature selection within tuning step three algorithms. Set rfe = TRUE splendid use feature selection. One limitation RFE priori set feature subset sizes need specified, determining search space algorithm. default, set sizes parameter every 5 integers 0 one-half smallest class size. Recall class sizes: tell RFE search best models 5, 10, …, 50 features. Cross-validation 2 folds used algorithm.","code":"table(class) #> class #> DIF.C4 IMM.C2 MES.C1 PRO.C5  #>    135    107    109    138"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"prediction","dir":"Articles","previous_headings":"","what":"Prediction","title":"Ensemble Classification using `splendid`","text":"different prediction method classifier, since imported external package. prediction() function calls method based class model output classification(). prediction() also performs manipulations results data structure: unnamed factors, labels given order true class labels. method \"pam\" exception: output prediction() predicted class labels, also cross-validated threshold value calculated training set use prediction test set, named delta.","code":"str(sl_result$preds, max.level = 2, list.len = 2) #> List of 3 #>  $ slda:List of 2 #>   ..$ : Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 2 1 1 1 1 1 1 4 ... #>   .. ..- attr(*, \"prob\")= num [1:178, 1:4] 0 0 0.0037 1 1 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. ..- attr(*, \"class.true\")= Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 2 1 1 1 4 1 2 4 ... #>   .. ..- attr(*, \"class.thres\")= Factor w/ 5 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 2 1 1 1 1 1 1 4 ... #>   .. ..- attr(*, \"class.prop\")= num 1 #>   ..$ : Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 1 2 4 1 1 1 1 1 ... #>   .. ..- attr(*, \"prob\")= num [1:161, 1:4] 0.002975 0.000202 0.999009 0.236365 0 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. ..- attr(*, \"class.true\")= Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 4 2 4 4 1 1 1 1 ... #>   .. ..- attr(*, \"class.thres\")= Factor w/ 5 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 1 2 4 1 1 1 1 1 ... #>   .. ..- attr(*, \"class.prop\")= num 1 #>  $ knn :List of 2 #>   ..$ : Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 3 3 2 1 1 4 4 1 2 4 ... #>   .. ..- attr(*, \"prob\")= num [1:178, 1:4] 0.2 0 0 1 1 0.4 0 1 0.4 0 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. ..- attr(*, \"class.true\")= Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 2 1 1 1 4 1 2 4 ... #>   .. ..- attr(*, \"class.thres\")= Factor w/ 5 levels \"DIF.C4\",\"IMM.C2\",..: 3 3 2 1 1 4 4 1 2 4 ... #>   .. ..- attr(*, \"class.prop\")= num 1 #>   ..$ : Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 1 3 1 2 4 1 1 1 2 2 ... #>   .. ..- attr(*, \"prob\")= num [1:161, 1:4] 0.6 0.167 0.667 0 0 ... #>   .. .. ..- attr(*, \"dimnames\")=List of 2 #>   .. ..- attr(*, \"class.true\")= Factor w/ 4 levels \"DIF.C4\",\"IMM.C2\",..: 4 3 4 2 4 4 1 1 1 1 ... #>   .. ..- attr(*, \"class.thres\")= Factor w/ 5 levels \"DIF.C4\",\"IMM.C2\",..: 1 3 1 2 4 1 1 1 1 2 ... #>   .. ..- attr(*, \"class.prop\")= num 1 #>   [list output truncated]"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"attributes","dir":"Articles","previous_headings":"Prediction","what":"Attributes","title":"Ensemble Classification using `splendid`","text":"Note predicted class attribute shows class probabilities matrix. ensure class probabilities every sample sum one making small adjustment one classes depending whether probability one. matrices useful compute evaluation measures generate discriminating graphs detail , stored attr(*, \"prob\"). obtain better performance evaluation metrics, may want exclude samples maximum class probability threshold. example, may compare true test labels corresponding OOB samples winning class probability least 50%. Samples threshold labelled \"unclassified\". threshold results samples unclassified, use unfiltered, original predicted labels evaluation. object stored attr(*, \"class.thres\"). final attribute prediction object stored attr(*, \"class.prop\"), showing proportion classified cases. value inversely proportional threshold setting.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"metrics","dir":"Articles","previous_headings":"Evaluation","what":"Metrics","title":"Ensemble Classification using `splendid`","text":"Evaluation measures important tell us prediction performance classifier. table shows measures svm, two bootstrap replicates. logloss, auc pdi measures make use prediction probabilities, whereas rest computed multiclass confusion matrix reference predicted class labels. SVM Evaluation Metrics logloss: Multiclass log loss minimized auc: Area Curve compares trade-sensitivity specificity pdi: Polytomous Discrimination Index accuracy: number correctly classified samples samples ppv: positive predictive value, also known precision npv: negative predictive value sensitivity: also known recall specificity: also known selectivity f1: balanced metric precision recall mcc: Matthew’s correlation coefficient Note multiclass case, variants measures listed , depending looking data. macro-averaged metric calculates said metric one-vs-confusion matrices takes mean. micro-averaged metric calculated element-wise sum one-vs-confusion matrices. Hence, macro_ppv, macro_npv, macro_sensitivity, macro_specificity, macro_f1 macro-averaged metrics, micro_mcc “” micro-averaged metric. turns accuracy mathematically equivalent micro-averaged PPV, sensitivity, F1-score, don’t redundantly add list. Furthermore, calculating mcc macro-averaged way recommended, use full confusion matrix. Class-specific measures fill rest matrix, labelled naming scheme metric.class. .632 estimator implemented multiclass log loss function. error estimate aims make compromise overbiased prediction like leave-one-bootstrap error underbiased prediction like training (apparent) error. set plus = TRUE splendid() (default), one can calculate .632+ estimator, improvement .632 estimator takes account amount overfitting.5 value stored attribute evaluation object:","code":"vapply(sl_result$evals, attr, \"err_632plus\", FUN.VALUE = double(1)) #>      slda       knn       svm  #> 1.8921645 1.4202706 0.2089945"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"plots","dir":"Articles","previous_headings":"Evaluation","what":"Plots","title":"Ensemble Classification using `splendid`","text":"assess performance classifier, can look discriminating plot reliability plot.","code":"data(hgsc) class <- factor(attr(hgsc, \"class.true\")) set.seed(1) training.id <- sample(seq_along(class), replace = TRUE) test.id <- which(!seq_along(class) %in% training.id) mod <- classification(hgsc[training.id, ], class[training.id], \"rf\") pred <- prediction(mod, hgsc, test.id, class = class) evals <- evaluation(class[test.id], pred, plot = TRUE)"},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"ensemble-construction","dir":"Articles","previous_headings":"","what":"Ensemble Construction","title":"Ensemble Classification using `splendid`","text":"bootstrap replicate training set, want find top performing classifier. use Rank Aggregation Genetic Algorithm choose top algorithm comparing across evaluation metrics4. case log loss, need first invert value since objective function minimization. Class-specific measures included rank aggregation interdependent. example, sample true class “” treated differently depending whether misclassified “B” “C”. obtaining list top classifiers bootstrap replicate, sort decreasing frequency keep top 3 use ensemble. ensemble_mods output splendid list models fit full data top classifiers.","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/ensemble_classification.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Ensemble Classification using `splendid`","text":"Ahdesmäki, Miika, Korbinian Strimmer. “Feature selection omics prediction problems using cat scores false nondiscovery rate control.” Annals Applied Statistics 4.1 (2010): 503-519. https://web.stanford.edu/~hastie/glmnet/glmnet_beta.html https://topepo.github.io/caret/recursive-feature-elimination.html Pihur, Vasyl, Susmita Datta, Somnath Datta. “RankAggreg, R package weighted rank aggregation.” BMC bioinformatics 10.1 (2009): 62. Efron, Bradley Tibshirani, Robert (1997), “Improvements Cross-Validation: .632+ Bootstrap Method,” Journal American Statistical Association, 92, 438, 548-560.","code":""},{"path":"https://alinetalhouk.github.io/splendid/articles/model_extensions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model Extensions","text":"splendid supports two modelling extensions ensemble classification performed. paradigms constructed set binary classifiers. first extension called one-vs-approach, class compared complement. second extension utilizes sequential algorithm build ensemble classifier, assigning class membership sequentially, order highest performance. goal provide user additional options flexibility modelling, since guarantee prediction accuracy improved.","code":"# install.packages(\"devtools\") # devtools::install_github(\"AlineTalhouk/splendid\") library(splendid) library(knitr) data(hgsc)"},{"path":"https://alinetalhouk.github.io/splendid/articles/model_extensions.html","id":"one-vs-all","dir":"Articles","previous_headings":"","what":"One-Vs-All","title":"Model Extensions","text":"build one-vs-model, ova model short, first need create binarized class matrix, class compared complement. Take hgsc data set example. first 10 classes predicted TCGA : corresponding binarized class matrix shown Table 1. denote \"0\" meaning “equal” class indicated column name. Table 1: Binarized class matrix ova Classification proceeds fashion every bootstrap replicate algorithm, specified splendid(). However, whereas one model fit, now four model fits iteration. Note k-Nearest Neighbour classification gives results using multiclass ova approach nonparametric nature. bit data manipulation required obtaining predicted class probability matrix. Let K number classes N number samples classify. K fits, N 2 probability matrix classifying k vs. k, k = 1, …, K. extract column probabilities pertaining k fit construct N K probability matrix. course, sum probabilities sample likely equate one. normalize probability matrix, divide row sum. Finally, ova predicted class taken largest probability row. ova approach hence produce probability matrix similar pattern conventional, multiclass approach less extreme values. results potentially unclassified cases choose threshold predictions.","code":"class <- attr(hgsc, \"class.true\") head(class, 10) #>  [1] \"PRO.C5\" \"MES.C1\" \"DIF.C4\" \"MES.C1\" \"MES.C1\" \"PRO.C5\" \"DIF.C4\" \"MES.C1\" #>  [9] \"IMM.C2\" \"PRO.C5\""},{"path":"https://alinetalhouk.github.io/splendid/articles/model_extensions.html","id":"sequential-algorithm","dir":"Articles","previous_headings":"","what":"Sequential Algorithm","title":"Model Extensions","text":"sequential algorithm follows complicated workflow. ensemble method, meaning work fitted models, predictions, evaluation metrics, whereas ova approach integrated initial classification. sequential, idea want sequentially assign classes binary fashion, classes assigned. diagram simple example 4-class case. Notice 4-class case, 3 fits (since last binary classification class “2” class “1”). determine sequence classes assigned? classify 3, 4, 1 vs. 2 example ? use per-class F1-score evaluation metric. F1-score shown robust traditional measures accuracy, represents balance precision recall. Suppose fit set classifiers 2 bootstrap replicates xgboost rf algorithms. Table 2 shows per-class F1-scores model fitted bootstrap replicate. logical column indicates whether ova used particular algorithm. Table 2: Per-class F1-scores model bootstrap replicate calculate average value across model boot obtain mean F1-score across algorithms replicates. model associated maximum mean F1-score certain class used sequential algorithm. Finally, sequence fits determined descending magnitude maximum mean F1-score. Table 3 shows class “MES.C1” highest mean F1-score, model produced rf, . means first fit “MES.C1” vs. “MES.C1” using rf, “IMM.C2” vs. “IMM.C2” using xgboost, finally “PRO.C5” vs. “DIF.C4” using rf. information last row Table 3 simply reference. Table 3: Ranked classes associated models slightly different binarized class matrix needed result: Table 4: Binarized class matrix sequential algorithm Table 4 similar nature Table 1, except two “worst-classified” classes grouped together (.e. “PRO.C5” “DIF.C4”). sequential fit, nonzero rows removed entire matrix classified class column removed. means final sequential fit, matrix reduces single column “PRO.C5” “DIF.C4” values. prediction output generated applying sequential algorithm full data. can extract binarized probability matrices also 2--2 confusion matrices.","code":"sm <- splendid_model(hgsc, class, n = 2, algorithms = c(\"xgboost\", \"rf\"))"},{"path":"https://alinetalhouk.github.io/splendid/articles/model_extensions.html","id":"usage","dir":"Articles","previous_headings":"","what":"Usage","title":"Model Extensions","text":"ova approach can used splendid() setting ova = TRUE. every algorithm specified algorithms, analogous ova version implemented. pertinent results labelled string ova_algorithm_name. sequential algorithm can used construct ensemble classifier setting sequential = TRUE splendid(). Please read function documentation details.","code":""},{"path":"https://alinetalhouk.github.io/splendid/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Derek Chiu. Author, maintainer. Aline Talhouk. Author. Dustin Johnson. Author.","code":""},{"path":"https://alinetalhouk.github.io/splendid/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Chiu D, Talhouk , Johnson D (2025). splendid: SuPervised Learning ENsemble Diagnostic IDentification. R package version 0.4.2, https://github.com/AlineTalhouk/splendid.","code":"@Manual{,   title = {splendid: SuPervised Learning ENsemble for Diagnostic IDentification},   author = {Derek Chiu and Aline Talhouk and Dustin Johnson},   year = {2025},   note = {R package version 0.4.2},   url = {https://github.com/AlineTalhouk/splendid}, }"},{"path":[]},{"path":"https://alinetalhouk.github.io/splendid/index.html","id":"overview","dir":"","previous_headings":"","what":"Overview","title":"SuPervised Learning ENsemble for Diagnostic IDentification","text":"goal splendid provide supervised learning pipeline implements major components multiclass classification problem. guide user fitting classifier, obtaining predictions, ultimately evaluating performance using metrics visualizations.","code":""},{"path":"https://alinetalhouk.github.io/splendid/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"SuPervised Learning ENsemble for Diagnostic IDentification","text":"can install splendid github :","code":"# install.packages(\"devtools\") devtools::install_github(\"AlineTalhouk/splendid\")"},{"path":"https://alinetalhouk.github.io/splendid/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"SuPervised Learning ENsemble for Diagnostic IDentification","text":"following example shows use main function package, splendid(). data matrix hgsc contains subset gene expression measurements High Grade Serous Ovarian Carcinoma patients Cancer Genome Atlas publicly available datasets. Samples rows, features columns. function runs package splendid() function. First extract reference class labels (TCGA) row names hgsc. fit random forest extreme gradient boosting classifiers one bootstrapped replicate data.","code":"library(splendid) data(hgsc) class <- attr(hgsc, \"class.true\") sl_result <- splendid(data = hgsc, class = class, n = 1,                       algorithms = c(\"rf\", \"xgboost\"), seed_boot = 5) str(sl_result, max.level = 2) #> List of 8 #>  $ models       :List of 2 #>   ..$ rf     :List of 1 #>   ..$ xgboost:List of 1 #>  $ preds        :List of 2 #>   ..$ rf     :List of 1 #>   ..$ xgboost:List of 1 #>  $ evals        :List of 2 #>   ..$ rf     :'data.frame':  48 obs. of  1 variable: #>   .. ..- attr(*, \"err_632plus\")= num 0.433 #>   ..$ xgboost:'data.frame':  48 obs. of  1 variable: #>   .. ..- attr(*, \"err_632plus\")= num 0.858 #>  $ bests        : Named chr [1:2] \"rf\" \"rf\" #>   ..- attr(*, \"names\")= chr [1:2] \"1\" \"X1\" #>  $ ensemble_algs: chr \"rf\" #>  $ ensemble_mods:List of 1 #>   ..$ :List of 18 #>   .. ..- attr(*, \"class\")= chr \"randomForest\" #>  $ seq_mods     : NULL #>  $ seq_preds    : NULL"},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_test.html","id":null,"dir":"Reference","previous_headings":"","what":"Obtain OOB sample to use as test set — boot_test","title":"Obtain OOB sample to use as test set — boot_test","text":"Obtain OOB sample use test set","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_test.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Obtain OOB sample to use as test set — boot_test","text":"","code":"boot_test(train.id)"},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_test.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Obtain OOB sample to use as test set — boot_test","text":"train.id list training set indices","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Recursively create training set indices ensuring class representation in every bootstrap resample — boot_train","title":"Recursively create training set indices ensuring class representation in every bootstrap resample — boot_train","text":"Recursively create training set indices ensuring class representation every bootstrap resample","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Recursively create training set indices ensuring class representation in every bootstrap resample — boot_train","text":"","code":"boot_train(data, class, n, stratify = FALSE)"},{"path":"https://alinetalhouk.github.io/splendid/reference/boot_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Recursively create training set indices ensuring class representation in every bootstrap resample — boot_train","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning n number bootstrap replicates generate stratify logical; TRUE, bootstrap resampling performed within strata class ensure bootstrap sample contains proportions strata original data.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":null,"dir":"Reference","previous_headings":"","what":"Multiclass classification — classification","title":"Multiclass classification — classification","text":"Run multiclass classification algorithm given dataset reference class.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Multiclass classification — classification","text":"","code":"classification(   data,   class,   algorithms,   rfe = FALSE,   ova = FALSE,   standardize = FALSE,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   seed_samp = NULL,   sizes = NULL,   trees = 100,   tune = FALSE,   seed_alg = NULL,   convert = FALSE )"},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Multiclass classification — classification","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithms character string algorithm use supervised learning. See Algorithms section possible options. rfe logical; TRUE, run Recursive Feature Elimination feature selection method \"lda\", \"rf\", \"svm\" algorithms. ova logical; TRUE, use One-Vs-approach knn algorithm. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. seed_samp random seed used reproducibility subsampling training sets model generation sizes range sizes features test RFE algorithm trees number trees use \"rf\" tune logical; TRUE, algorithms hyperparameters tuned seed_alg random seed used reproducibility running algorithms intrinsic random element (random forests) convert logical; TRUE, converts categorical variables data dummy variables. Certain algorithms work limitations (e.g. LDA).","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Multiclass classification — classification","text":"model object running classification algorithm","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Multiclass classification — classification","text":"classification algorithms implemented use pre-defined values specify settings options others need tune hyperparameters. \"multinom\" \"nnet\" use maximum number weights 2000, case data high dimensional classification time-consuming. \"nnet\" also tunes number nodes (1-5) hidden layer. \"pam\" considers 100 thresholds training, uses uniform prior. \"adaboost_m1\" calls adabag::boosting() supports hyperparameter tuning. alg = \"knn\", return value NULL class::knn() output intermediate model object. modelling prediction performed one step. However, class attribute \"knn\" still assigned result order call respective prediction() method. additional class \"ova\" added ova = TRUE.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"algorithms","dir":"Reference","previous_headings":"","what":"Algorithms","title":"Multiclass classification — classification","text":"classification algorithms currently supported : Prediction Analysis Microarrays (\"pam\") Support Vector Machines (\"svm\") Random Forests (\"rf\") Linear Discriminant Analysis (\"lda\") Shrinkage Linear Discriminant Analysis (\"slda\") Shrinkage Diagonal Discriminant Analysis (\"sdda\") Multinomial Logistic Regression using Generalized Linear Model penalization (\"mlr_glm\") GLM LASSO penalty (\"mlr_lasso\") GLM ridge penalty (\"mlr_ridge\") GLM elastic net penalty (\"mlr_enet\") Neural Networks (\"mlr_nnet\") Neural Networks (\"nnet\") Naive Bayes (\"nbayes\") AdaBoost.M1 (\"adaboost_m1\") Extreme Gradient Boosting (\"xgboost\") K-Nearest Neighbours (\"knn\")","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Multiclass classification — classification","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/classification.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Multiclass classification — classification","text":"","code":"data(hgsc) class <- attr(hgsc, \"class.true\") classification(hgsc, class, \"xgboost\") #> ##### xgb.Booster #> raw: 18.6 Kb  #> call: #>   xgboost::xgb.train(params = list(objective = \"multi:softprob\",  #>     eval_metric = \"mlogloss\", num_class = nlevels(class)), data = xgboost::xgb.DMatrix(data = as.matrix(data),  #>     label = as.integer(class) - 1), nrounds = 2) #> params (as set within xgb.train): #>   objective = \"multi:softprob\", eval_metric = \"mlogloss\", num_class = \"4\", validate_parameters = \"TRUE\" #> xgb.attributes: #>   niter #> callbacks: #>   cb.print.evaluation(period = print_every_n) #> # of features: 321  #> niter: 2 #> nfeatures : 321"},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":null,"dir":"Reference","previous_headings":"","what":"Create dummy variables — dummify","title":"Create dummy variables — dummify","text":"Convert categorical variables dataset dummy variables","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create dummy variables — dummify","text":"","code":"dummify(data)"},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create dummy variables — dummify","text":"data data frame rows samples, columns features","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create dummy variables — dummify","text":"numeric data frame dummy variables encode categorical variables using multiple columns. Continuous variables unchanged.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Create dummy variables — dummify","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/dummify.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create dummy variables — dummify","text":"","code":"dummify(mtcars) #>     mpg cyl  disp  hp drat    wt  qsec vs am gear carb #> 1  21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4 #> 2  21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4 #> 3  22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1 #> 4  21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1 #> 5  18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2 #> 6  18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1 #> 7  14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4 #> 8  24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2 #> 9  22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2 #> 10 19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4 #> 11 17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4 #> 12 16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3 #> 13 17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3 #> 14 15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3 #> 15 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4 #> 16 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4 #> 17 14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4 #> 18 32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1 #> 19 30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2 #> 20 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1 #> 21 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1 #> 22 15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2 #> 23 15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2 #> 24 13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4 #> 25 19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2 #> 26 27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1 #> 27 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2 #> 28 30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2 #> 29 15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4 #> 30 19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6 #> 31 15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8 #> 32 21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2 dummify(iris) #>     Sepal.Length Sepal.Width Petal.Length Petal.Width Speciesversicolor #> 1            5.1         3.5          1.4         0.2                 0 #> 2            4.9         3.0          1.4         0.2                 0 #> 3            4.7         3.2          1.3         0.2                 0 #> 4            4.6         3.1          1.5         0.2                 0 #> 5            5.0         3.6          1.4         0.2                 0 #> 6            5.4         3.9          1.7         0.4                 0 #> 7            4.6         3.4          1.4         0.3                 0 #> 8            5.0         3.4          1.5         0.2                 0 #> 9            4.4         2.9          1.4         0.2                 0 #> 10           4.9         3.1          1.5         0.1                 0 #> 11           5.4         3.7          1.5         0.2                 0 #> 12           4.8         3.4          1.6         0.2                 0 #> 13           4.8         3.0          1.4         0.1                 0 #> 14           4.3         3.0          1.1         0.1                 0 #> 15           5.8         4.0          1.2         0.2                 0 #> 16           5.7         4.4          1.5         0.4                 0 #> 17           5.4         3.9          1.3         0.4                 0 #> 18           5.1         3.5          1.4         0.3                 0 #> 19           5.7         3.8          1.7         0.3                 0 #> 20           5.1         3.8          1.5         0.3                 0 #> 21           5.4         3.4          1.7         0.2                 0 #> 22           5.1         3.7          1.5         0.4                 0 #> 23           4.6         3.6          1.0         0.2                 0 #> 24           5.1         3.3          1.7         0.5                 0 #> 25           4.8         3.4          1.9         0.2                 0 #> 26           5.0         3.0          1.6         0.2                 0 #> 27           5.0         3.4          1.6         0.4                 0 #> 28           5.2         3.5          1.5         0.2                 0 #> 29           5.2         3.4          1.4         0.2                 0 #> 30           4.7         3.2          1.6         0.2                 0 #> 31           4.8         3.1          1.6         0.2                 0 #> 32           5.4         3.4          1.5         0.4                 0 #> 33           5.2         4.1          1.5         0.1                 0 #> 34           5.5         4.2          1.4         0.2                 0 #> 35           4.9         3.1          1.5         0.2                 0 #> 36           5.0         3.2          1.2         0.2                 0 #> 37           5.5         3.5          1.3         0.2                 0 #> 38           4.9         3.6          1.4         0.1                 0 #> 39           4.4         3.0          1.3         0.2                 0 #> 40           5.1         3.4          1.5         0.2                 0 #> 41           5.0         3.5          1.3         0.3                 0 #> 42           4.5         2.3          1.3         0.3                 0 #> 43           4.4         3.2          1.3         0.2                 0 #> 44           5.0         3.5          1.6         0.6                 0 #> 45           5.1         3.8          1.9         0.4                 0 #> 46           4.8         3.0          1.4         0.3                 0 #> 47           5.1         3.8          1.6         0.2                 0 #> 48           4.6         3.2          1.4         0.2                 0 #> 49           5.3         3.7          1.5         0.2                 0 #> 50           5.0         3.3          1.4         0.2                 0 #> 51           7.0         3.2          4.7         1.4                 1 #> 52           6.4         3.2          4.5         1.5                 1 #> 53           6.9         3.1          4.9         1.5                 1 #> 54           5.5         2.3          4.0         1.3                 1 #> 55           6.5         2.8          4.6         1.5                 1 #> 56           5.7         2.8          4.5         1.3                 1 #> 57           6.3         3.3          4.7         1.6                 1 #> 58           4.9         2.4          3.3         1.0                 1 #> 59           6.6         2.9          4.6         1.3                 1 #> 60           5.2         2.7          3.9         1.4                 1 #> 61           5.0         2.0          3.5         1.0                 1 #> 62           5.9         3.0          4.2         1.5                 1 #> 63           6.0         2.2          4.0         1.0                 1 #> 64           6.1         2.9          4.7         1.4                 1 #> 65           5.6         2.9          3.6         1.3                 1 #> 66           6.7         3.1          4.4         1.4                 1 #> 67           5.6         3.0          4.5         1.5                 1 #> 68           5.8         2.7          4.1         1.0                 1 #> 69           6.2         2.2          4.5         1.5                 1 #> 70           5.6         2.5          3.9         1.1                 1 #> 71           5.9         3.2          4.8         1.8                 1 #> 72           6.1         2.8          4.0         1.3                 1 #> 73           6.3         2.5          4.9         1.5                 1 #> 74           6.1         2.8          4.7         1.2                 1 #> 75           6.4         2.9          4.3         1.3                 1 #> 76           6.6         3.0          4.4         1.4                 1 #> 77           6.8         2.8          4.8         1.4                 1 #> 78           6.7         3.0          5.0         1.7                 1 #> 79           6.0         2.9          4.5         1.5                 1 #> 80           5.7         2.6          3.5         1.0                 1 #> 81           5.5         2.4          3.8         1.1                 1 #> 82           5.5         2.4          3.7         1.0                 1 #> 83           5.8         2.7          3.9         1.2                 1 #> 84           6.0         2.7          5.1         1.6                 1 #> 85           5.4         3.0          4.5         1.5                 1 #> 86           6.0         3.4          4.5         1.6                 1 #> 87           6.7         3.1          4.7         1.5                 1 #> 88           6.3         2.3          4.4         1.3                 1 #> 89           5.6         3.0          4.1         1.3                 1 #> 90           5.5         2.5          4.0         1.3                 1 #> 91           5.5         2.6          4.4         1.2                 1 #> 92           6.1         3.0          4.6         1.4                 1 #> 93           5.8         2.6          4.0         1.2                 1 #> 94           5.0         2.3          3.3         1.0                 1 #> 95           5.6         2.7          4.2         1.3                 1 #> 96           5.7         3.0          4.2         1.2                 1 #> 97           5.7         2.9          4.2         1.3                 1 #> 98           6.2         2.9          4.3         1.3                 1 #> 99           5.1         2.5          3.0         1.1                 1 #> 100          5.7         2.8          4.1         1.3                 1 #> 101          6.3         3.3          6.0         2.5                 0 #> 102          5.8         2.7          5.1         1.9                 0 #> 103          7.1         3.0          5.9         2.1                 0 #> 104          6.3         2.9          5.6         1.8                 0 #> 105          6.5         3.0          5.8         2.2                 0 #> 106          7.6         3.0          6.6         2.1                 0 #> 107          4.9         2.5          4.5         1.7                 0 #> 108          7.3         2.9          6.3         1.8                 0 #> 109          6.7         2.5          5.8         1.8                 0 #> 110          7.2         3.6          6.1         2.5                 0 #> 111          6.5         3.2          5.1         2.0                 0 #> 112          6.4         2.7          5.3         1.9                 0 #> 113          6.8         3.0          5.5         2.1                 0 #> 114          5.7         2.5          5.0         2.0                 0 #> 115          5.8         2.8          5.1         2.4                 0 #> 116          6.4         3.2          5.3         2.3                 0 #> 117          6.5         3.0          5.5         1.8                 0 #> 118          7.7         3.8          6.7         2.2                 0 #> 119          7.7         2.6          6.9         2.3                 0 #> 120          6.0         2.2          5.0         1.5                 0 #> 121          6.9         3.2          5.7         2.3                 0 #> 122          5.6         2.8          4.9         2.0                 0 #> 123          7.7         2.8          6.7         2.0                 0 #> 124          6.3         2.7          4.9         1.8                 0 #> 125          6.7         3.3          5.7         2.1                 0 #> 126          7.2         3.2          6.0         1.8                 0 #> 127          6.2         2.8          4.8         1.8                 0 #> 128          6.1         3.0          4.9         1.8                 0 #> 129          6.4         2.8          5.6         2.1                 0 #> 130          7.2         3.0          5.8         1.6                 0 #> 131          7.4         2.8          6.1         1.9                 0 #> 132          7.9         3.8          6.4         2.0                 0 #> 133          6.4         2.8          5.6         2.2                 0 #> 134          6.3         2.8          5.1         1.5                 0 #> 135          6.1         2.6          5.6         1.4                 0 #> 136          7.7         3.0          6.1         2.3                 0 #> 137          6.3         3.4          5.6         2.4                 0 #> 138          6.4         3.1          5.5         1.8                 0 #> 139          6.0         3.0          4.8         1.8                 0 #> 140          6.9         3.1          5.4         2.1                 0 #> 141          6.7         3.1          5.6         2.4                 0 #> 142          6.9         3.1          5.1         2.3                 0 #> 143          5.8         2.7          5.1         1.9                 0 #> 144          6.8         3.2          5.9         2.3                 0 #> 145          6.7         3.3          5.7         2.5                 0 #> 146          6.7         3.0          5.2         2.3                 0 #> 147          6.3         2.5          5.0         1.9                 0 #> 148          6.5         3.0          5.2         2.0                 0 #> 149          6.2         3.4          5.4         2.3                 0 #> 150          5.9         3.0          5.1         1.8                 0 #>     Speciesvirginica #> 1                  0 #> 2                  0 #> 3                  0 #> 4                  0 #> 5                  0 #> 6                  0 #> 7                  0 #> 8                  0 #> 9                  0 #> 10                 0 #> 11                 0 #> 12                 0 #> 13                 0 #> 14                 0 #> 15                 0 #> 16                 0 #> 17                 0 #> 18                 0 #> 19                 0 #> 20                 0 #> 21                 0 #> 22                 0 #> 23                 0 #> 24                 0 #> 25                 0 #> 26                 0 #> 27                 0 #> 28                 0 #> 29                 0 #> 30                 0 #> 31                 0 #> 32                 0 #> 33                 0 #> 34                 0 #> 35                 0 #> 36                 0 #> 37                 0 #> 38                 0 #> 39                 0 #> 40                 0 #> 41                 0 #> 42                 0 #> 43                 0 #> 44                 0 #> 45                 0 #> 46                 0 #> 47                 0 #> 48                 0 #> 49                 0 #> 50                 0 #> 51                 0 #> 52                 0 #> 53                 0 #> 54                 0 #> 55                 0 #> 56                 0 #> 57                 0 #> 58                 0 #> 59                 0 #> 60                 0 #> 61                 0 #> 62                 0 #> 63                 0 #> 64                 0 #> 65                 0 #> 66                 0 #> 67                 0 #> 68                 0 #> 69                 0 #> 70                 0 #> 71                 0 #> 72                 0 #> 73                 0 #> 74                 0 #> 75                 0 #> 76                 0 #> 77                 0 #> 78                 0 #> 79                 0 #> 80                 0 #> 81                 0 #> 82                 0 #> 83                 0 #> 84                 0 #> 85                 0 #> 86                 0 #> 87                 0 #> 88                 0 #> 89                 0 #> 90                 0 #> 91                 0 #> 92                 0 #> 93                 0 #> 94                 0 #> 95                 0 #> 96                 0 #> 97                 0 #> 98                 0 #> 99                 0 #> 100                0 #> 101                1 #> 102                1 #> 103                1 #> 104                1 #> 105                1 #> 106                1 #> 107                1 #> 108                1 #> 109                1 #> 110                1 #> 111                1 #> 112                1 #> 113                1 #> 114                1 #> 115                1 #> 116                1 #> 117                1 #> 118                1 #> 119                1 #> 120                1 #> 121                1 #> 122                1 #> 123                1 #> 124                1 #> 125                1 #> 126                1 #> 127                1 #> 128                1 #> 129                1 #> 130                1 #> 131                1 #> 132                1 #> 133                1 #> 134                1 #> 135                1 #> 136                1 #> 137                1 #> 138                1 #> 139                1 #> 140                1 #> 141                1 #> 142                1 #> 143                1 #> 144                1 #> 145                1 #> 146                1 #> 147                1 #> 148                1 #> 149                1 #> 150                1"},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":null,"dir":"Reference","previous_headings":"","what":".632(+) Estimator for log loss error rate — error_632","title":".632(+) Estimator for log loss error rate — error_632","text":".632 estimator log loss error rate calculated given classifier. .632+ estimator extension reduces overfitting run default.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":".632(+) Estimator for log loss error rate — error_632","text":"","code":"error_632(data, class, algorithm, pred, test.id, train.id, plus = TRUE)"},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":".632(+) Estimator for log loss error rate — error_632","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithm character string classifier. See splendid possible options. pred vector OOB predictions using classifier algorithm. test.id vector test set indices bootstrap replicate train.id vector training set indices bootstrap replicate plus logical; TRUE (default), .632+ estimator calculated. Otherwise, .632 estimator calculated.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":".632(+) Estimator for log loss error rate — error_632","text":".632(+) log loss error rate","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":".632(+) Estimator for log loss error rate — error_632","text":"function intended used internally splendid_model.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":".632(+) Estimator for log loss error rate — error_632","text":"Friedman, Jerome, Trevor Hastie, Robert Tibshirani. elements statistical learning. Vol. 1. New York: Springer series statistics, 2001. Efron, Bradley Tibshirani, Robert (1997), \"Improvements Cross-Validation: .632+ Bootstrap Method,\" Journal American Statistical Association, 92, 438, 548-560.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":".632(+) Estimator for log loss error rate — error_632","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/error_632.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":".632(+) Estimator for log loss error rate — error_632","text":"","code":"if (FALSE) { # \\dontrun{ data(hgsc) class <- as.factor(attr(hgsc, \"class.true\")) set.seed(1) train.id <- boot_train(data = hgsc, class = class, n = 5) test.id <- boot_test(train.id = train.id) mod <- purrr::map(train.id, ~ classification(hgsc[., ], class[.], \"xgboost\")) pred <- purrr::pmap(list(mod = mod, test.id = test.id, train.id = train.id), prediction, data = hgsc, class = class) error_632(hgsc, class, \"xgboost\", pred, test.id, train.id, plus = FALSE) error_632(hgsc, class, \"xgboost\", pred, test.id, train.id, plus = TRUE) } # }"},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":null,"dir":"Reference","previous_headings":"","what":"Evaluation of prediction performance — evaluation","title":"Evaluation of prediction performance — evaluation","text":"Evaluation prediction performance OOB set done using various measure classification problems.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Evaluation of prediction performance — evaluation","text":"","code":"evaluation(x, y, plot = FALSE)"},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Evaluation of prediction performance — evaluation","text":"x true class labels y predicted class labels plot logical; TRUE discrimination plot reliability plot shown class","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Evaluation of prediction performance — evaluation","text":"list one element per evaluation measure except cs element, returns list class-specific evaluation measures.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Evaluation of prediction performance — evaluation","text":"currently supported evaluation measures include discriminatory measures like log loss, AUC, PDI, macro-averaged PPV (Precision)/Sensitivity (Recall)/F1-score, accuracy (micro-averaged PPV Sensitivity/F1-score), Matthew's Correlation Coefficient (micro-averaged analog), Kappa, G-mean, class-specific accuracy/PPV/NPV/Sensitivity/Specificity/F1-score/MCC/Kappa/G-mean.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Evaluation of prediction performance — evaluation","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/evaluation.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Evaluation of prediction performance — evaluation","text":"","code":"data(hgsc) class <- factor(attr(hgsc, \"class.true\")) set.seed(1) training.id <- sample(seq_along(class), replace = TRUE) test.id <- which(!seq_along(class) %in% training.id) mod <- classification(hgsc[training.id, ], class[training.id], \"xgboost\") pred <- prediction(mod, hgsc, class, test.id) evaluation(class[test.id], pred) #> $logloss #> [1] 0.9125752 #>  #> $auc #> [1] 0.9372867 #>  #> $pdi #> [1] 0.8493226 #>  #> $accuracy #> [1] 0.7540107 #>  #> $macro_ppv #> [1] 0.7578747 #>  #> $macro_npv #> [1] 0.9177646 #>  #> $macro_sensitivity #> [1] 0.758613 #>  #> $macro_specificity #> [1] 0.9176079 #>  #> $macro_f1 #> [1] 0.7560955 #>  #> $mcc #> [1] 0.6726097 #>  #> $kappa #> [1] 0.6714537 #>  #> $gmean #> [1] 0.7548308 #>  #> $cs #>    accuracy.DIF.C4    accuracy.IMM.C2    accuracy.MES.C1    accuracy.PRO.C5  #>          0.8235294          0.8770053          0.9251337          0.8823529  #>         ppv.DIF.C4         ppv.IMM.C2         ppv.MES.C1         ppv.PRO.C5  #>          0.6851852          0.7045455          0.8125000          0.8292683  #>         npv.DIF.C4         npv.IMM.C2         npv.MES.C1         npv.PRO.C5  #>          0.8796992          0.9300699          0.9640288          0.8972603  #> sensitivity.DIF.C4 sensitivity.IMM.C2 sensitivity.MES.C1 sensitivity.PRO.C5  #>          0.6981132          0.7560976          0.8863636          0.6938776  #> specificity.DIF.C4 specificity.IMM.C2 specificity.MES.C1 specificity.PRO.C5  #>          0.8731343          0.9109589          0.9370629          0.9492754  #>          f1.DIF.C4          f1.IMM.C2          f1.MES.C1          f1.PRO.C5  #>          0.6915888          0.7294118          0.8478261          0.7555556  #>         mcc.DIF.C4         mcc.IMM.C2         mcc.MES.C1         mcc.PRO.C5  #>          0.5680571          0.6506338          0.7996339          0.6835707  #>       kappa.DIF.C4       kappa.IMM.C2       kappa.MES.C1       kappa.PRO.C5  #>          0.5680084          0.6499552          0.7983051          0.6788948  #>       gmean.DIF.C4       gmean.IMM.C2       gmean.MES.C1       gmean.PRO.C5  #>          0.7807347          0.8299240          0.9113608          0.8115916  #>"},{"path":"https://alinetalhouk.github.io/splendid/reference/hgsc.html","id":null,"dir":"Reference","previous_headings":"","what":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"489 samples measured 321 genes. Sample IDs row names gene names column names. data set used clustering HGSC subtypes prognostic significance. cluster assignments obtained TCGA indicated last six characters row name hgsc: MES.C1, IMM.C2, DIF.C4, PRO.C5","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/hgsc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"","code":"hgsc"},{"path":"https://alinetalhouk.github.io/splendid/reference/hgsc.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Gene expression data for High Grade Serous Carcinoma from TCGA — hgsc","text":"data frame 489 rows 321 columns.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_classification.html","id":null,"dir":"Reference","previous_headings":"","what":"One-Vs-All training approach — ova_classification","title":"One-Vs-All training approach — ova_classification","text":"One-Vs-training approach","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_classification.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-Vs-All training approach — ova_classification","text":"","code":"ova_classification(   data,   class,   algorithms,   rfe = FALSE,   ova = FALSE,   standardize = FALSE,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   seed_samp = NULL,   trees = 100,   tune = FALSE,   seed_alg = NULL )"},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_classification.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-Vs-All training approach — ova_classification","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithms character string algorithm use supervised learning. See Algorithms section possible options. rfe logical; TRUE, run Recursive Feature Elimination feature selection method \"lda\", \"rf\", \"svm\" algorithms. ova logical; TRUE, use One-Vs-approach knn algorithm. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. seed_samp random seed used reproducibility subsampling training sets model generation trees number trees use \"rf\" tune logical; TRUE, algorithms hyperparameters tuned seed_alg random seed used reproducibility running algorithms intrinsic random element (random forests)","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_classification.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"One-Vs-All training approach — ova_classification","text":"list binary classifier fits class","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_classification.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"One-Vs-All training approach — ova_classification","text":"Dustin Johnson, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"One-Vs-All prediction approach — ova_prediction","title":"One-Vs-All prediction approach — ova_prediction","text":"One-Vs-prediction approach","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-Vs-All prediction approach — ova_prediction","text":"","code":"ova_prediction(   fits,   data,   class,   test.id = NULL,   train.id = NULL,   threshold = 0,   standardize = FALSE,   ... )"},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-Vs-All prediction approach — ova_prediction","text":"fits list ova fits ova_classification data data frame rows samples, columns features class true/reference class vector used supervised learning test.id integer vector indices test set. NULL (default), samples used. train.id integer vector indices training set. NULL (default), samples used. threshold number 0 1 indicating lowest maximum class probability sample unclassified. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. ... additional arguments passed methods","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"One-Vs-All prediction approach — ova_prediction","text":"(tibble) predicted probabilities class","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/ova_prediction.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"One-Vs-All prediction approach — ova_prediction","text":"Dustin Johnson, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":null,"dir":"Reference","previous_headings":"","what":"Class prediction on OOB set — prediction","title":"Class prediction on OOB set — prediction","text":"Functions predict class labels --Bag (test) set different classifiers.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Class prediction on OOB set — prediction","text":"","code":"prediction(   mod,   data,   class = NULL,   test.id = NULL,   train.id = NULL,   threshold = 0,   standardize = FALSE,   ... )  # Default S3 method prediction(   mod,   data,   class = NULL,   test.id = NULL,   train.id = NULL,   threshold = 0,   standardize = FALSE,   ... )  # S3 method for class 'pamrtrained' prediction(   mod,   data,   class = NULL,   test.id = NULL,   train.id = NULL,   threshold = 0,   standardize = FALSE,   ... )  # S3 method for class 'knn' prediction(   mod,   data,   class = NULL,   test.id = NULL,   train.id = NULL,   threshold = 0,   standardize = FALSE,   ... )"},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Class prediction on OOB set — prediction","text":"mod model object classification() data data frame rows samples, columns features class true/reference class vector used supervised learning test.id integer vector indices test set. NULL (default), samples used. train.id integer vector indices training set. NULL (default), samples used. threshold number 0 1 indicating lowest maximum class probability sample unclassified. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. ... additional arguments passed methods","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Class prediction on OOB set — prediction","text":"factor predicted classes labels order true class. mod \"pamr\" classifier, return value list length 2: predicted class, threshold value.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Class prediction on OOB set — prediction","text":"knn pamr prediction methods use train.id class arguments additional modelling steps prediction. knn, modelling prediction performed one step, function takes training test set identifiers. pamr, classifier needs cross-validated training set order find shrinkage threshold minimum CV error use prediction test set. prediction methods make use default method.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Class prediction on OOB set — prediction","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/prediction.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Class prediction on OOB set — prediction","text":"","code":"data(hgsc) class <- attr(hgsc, \"class.true\") set.seed(1) training.id <- sample(seq_along(class), replace = TRUE) test.id <- which(!seq_along(class) %in% training.id) mod <- classification(hgsc[training.id, ], class[training.id], \"slda\") pred <- prediction(mod, hgsc, class, test.id) table(true = class[test.id], pred) #>         pred #> true     DIF.C4 IMM.C2 MES.C1 PRO.C5 #>   DIF.C4     43      4      3      3 #>   IMM.C2      5     33      3      0 #>   MES.C1      3      2     38      1 #>   PRO.C5     17      6      2     24"},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":null,"dir":"Reference","previous_headings":"","what":"Sequential Algorithm — sequential","title":"Sequential Algorithm — sequential","text":"Sequentially train top ranked algorithms class ordered class performance predict given class using sequentially trained fits.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sequential Algorithm — sequential","text":"","code":"sequential_train(sm, data, class, boxplot = FALSE)  sequential_pred(fit, sm, data, class, boxplot = FALSE)"},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sequential Algorithm — sequential","text":"sm splendid_model object data data frame rows samples, columns features class true/reference class vector used supervised learning boxplot TRUE, boxplots plotted showing distribution F1-scores per class, every algorithm. fit list fitted models sequential_train","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sequential Algorithm — sequential","text":"sequential_train returns list fits top-ranked sequence. sequential_pred returns list two elements prob: predicted sequential probabilities cm: confusion matrices class","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Sequential Algorithm — sequential","text":"sequential_train sequentially trains One-Vs-models classes classified. Hence n classes, n - 1 sequential fits. sequential_pred predicts class membership One-Vs-sequential model. Performance evaluated -class F1-scores, since better evaluation metrics accuracy, precision, recall.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Sequential Algorithm — sequential","text":"Dustin Johnson, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/sequential.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sequential Algorithm — sequential","text":"","code":"dat <- iris[, 1:4] class <- iris$Species sm <- splendid_model(dat, class, n = 2, algorithms = c(\"slda\", \"xgboost\")) st <- sequential_train(sm, dat, class) sp <- sequential_pred(st, sm, dat, class)"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid-package.html","id":null,"dir":"Reference","previous_headings":"","what":"splendid: SuPervised Learning ENsemble for Diagnostic IDentification — splendid-package","title":"splendid: SuPervised Learning ENsemble for Diagnostic IDentification — splendid-package","text":"Provides bootstrapping ensemble framework supervised learning analyses using multiclass classification algorithms modelling, prediction, evaluation. Predicted classes evaluated metrics log loss, AUC, F1-score, Matthew's correlation coefficient, accuracy. Discrimination reliability plots visualize classifier performances. .632+ estimator implemented log loss error rate.","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"splendid: SuPervised Learning ENsemble for Diagnostic IDentification — splendid-package","text":"Maintainer: Derek Chiu dchiu@bccrc.ca Authors: Aline Talhouk atalhouk@bccrc.ca Dustin Johnson djohnson@bccrc.ca","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":null,"dir":"Reference","previous_headings":"","what":"Ensemble framework for Supervised Learning classification problems — splendid","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"Supervised learning classification algorithms performed bootstrap replicates ensemble classifier built evaluated across variants.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"","code":"splendid(   data,   class,   algorithms = NULL,   n = 1,   seed_boot = NULL,   seed_samp = NULL,   seed_alg = NULL,   convert = FALSE,   rfe = FALSE,   ova = FALSE,   standardize = FALSE,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   stratify = FALSE,   plus = TRUE,   threshold = 0,   trees = 100,   tune = FALSE,   vi = FALSE,   top = 3,   seed_rank = 1,   sequential = FALSE )"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithms character vector algorithms use supervised learning. See Algorithms section possible options. default, argument NULL, case algorithms used. n number bootstrap replicates generate seed_boot random seed used reproducibility bootstrapping training sets model generation seed_samp random seed used reproducibility subsampling training sets model generation seed_alg random seed used reproducibility running algorithms intrinsic random element (random forests) convert logical; TRUE, converts categorical variables data dummy variables. Certain algorithms work limitations (e.g. LDA). rfe logical; TRUE, run Recursive Feature Elimination feature selection method \"lda\", \"rf\", \"svm\" algorithms. ova logical; TRUE, One-Vs-classification approach performed every algorithm algorithms. relevant results prefixed string ova_. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. stratify logical; TRUE, bootstrap resampling performed within strata class ensure bootstrap sample contains proportions strata original data. plus logical; TRUE (default), .632+ estimator calculated. Otherwise, .632 estimator calculated. threshold number 0 1 indicating lowest maximum class probability sample unclassified. trees number trees use \"rf\" tune logical; TRUE, algorithms hyperparameters tuned vi logical; TRUE, model-based variable importance scores returned algorithm available. Otherwise, SHAP-based VI scores calculated. top number highest-performing algorithms retain ensemble seed_rank random seed used reproducibility rank aggregation ensemble algorithms sequential logical; TRUE, sequential model fit algorithms best performance one-vs-classification.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"nested list five elements models: list element algorithm, list length n. Shows model object algorithm bootstrap replicate training set. preds: list element algorithm, list length n. Shows predicted classes algorithm bootstrap replicate test set. evals: bootstrap sample, can calculate various evaluation measures predicted classes algorithm. Evaluation measures include macro-averaged precision/recall/F1-score, micro-averaged precision, (micro-averaged MCC) return value eval tibble shows summary statistics (e.g. mean, median) evaluation measures across bootstrap samples, classification algorithm. bests: best-performing algorithm bootstrapped replicate data, chosen rank aggregation. ensemble_algs: tallies frequencies bests, returning top algorithms chosen. ensemble: list model fits algorithms ensemble_algs, fit full data.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"Training sets bootstrap replicates original data sampled replacement. Test sets comprise remaining samples left training set, also called --Bag samples. framework uses 0.632 bootstrap rule large n. ensemble classifier constructed using Rank Aggregation across multiple evaluation measures precision, recall, F1-score, Matthew's Correlation Coefficient (MCC).","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"algorithms","dir":"Reference","previous_headings":"","what":"Algorithms","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"classification algorithms currently supported : Prediction Analysis Microarrays (\"pam\") Support Vector Machines (\"svm\") Random Forests (\"rf\") Linear Discriminant Analysis (\"lda\") Shrinkage Linear Discriminant Analysis (\"slda\") Shrinkage Diagonal Discriminant Analysis (\"sdda\") Multinomial Logistic Regression using Generalized Linear Model penalization (\"mlr_glm\") GLM LASSO penalty (\"mlr_lasso\") GLM ridge penalty (\"mlr_ridge\") GLM elastic net penalty (\"mlr_enet\") Neural Networks (\"mlr_nnet\") Neural Networks (\"nnet\") Naive Bayes (\"nbayes\") AdaBoost.M1 (\"adaboost_m1\") Extreme Gradient Boosting (\"xgboost\") K-Nearest Neighbours (\"knn\")","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Ensemble framework for Supervised Learning classification problems — splendid","text":"","code":"if (FALSE) { # \\dontrun{ data(hgsc) class <- attr(hgsc, \"class.true\") sl_result <- splendid(hgsc, class, n = 2, algorithms = c(\"lda\", \"xgboost\")) } # }"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_ensemble.html","id":null,"dir":"Reference","previous_headings":"","what":"Combine classification models into an ensemble — splendid_ensemble","title":"Combine classification models into an ensemble — splendid_ensemble","text":"Combine classification models ensemble","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_ensemble.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Combine classification models into an ensemble — splendid_ensemble","text":"","code":"splendid_ensemble(   sm,   data,   class,   top = 3,   seed_rank = 1,   rfe = FALSE,   sequential = FALSE )"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_ensemble.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Combine classification models into an ensemble — splendid_ensemble","text":"sm splendid_model object data data frame rows samples, columns features class true/reference class vector used supervised learning top number highest-performing algorithms retain ensemble seed_rank random seed used reproducibility rank aggregation ensemble algorithms rfe logical; TRUE, run Recursive Feature Elimination feature selection method \"lda\", \"rf\", \"svm\" algorithms. sequential logical; TRUE, sequential model fit algorithms best performance one-vs-classification.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_ensemble.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Combine classification models into an ensemble — splendid_ensemble","text":"","code":"dat <- iris[, 1:4] class <- iris$Species sm <- splendid_model(dat, class, n = 3, algorithms = c(\"xgboost\", \"slda\")) se <- splendid_ensemble(sm, dat, class)"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":null,"dir":"Reference","previous_headings":"","what":"Discriminating graphs — splendid_graphs","title":"Discriminating graphs — splendid_graphs","text":"Graphs discrimination_plot, reliability_plot, roc_plot based true classes predicted class probabilities.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Discriminating graphs — splendid_graphs","text":"","code":"discrimination_plot(x, probs)  reliability_plot(x, probs)  roc_plot(x, probs)"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Discriminating graphs — splendid_graphs","text":"x true class labels probs matrix predicted class probabilities. Number rows must equal length x","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Discriminating graphs — splendid_graphs","text":"ggplot objects desired plot","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Discriminating graphs — splendid_graphs","text":"discrimination_plot shows boxplots predicted probabilities class, separated panels true class labels. class prevalence also drawn horizontal line panel. reliability_plot shows mean prediction vs. observed fraction lowess smoother class. line going thru origin slope 1 serves reference perfect reliability. roc_plot shows multi-class ROC curves class using 1-Specificity vs. Sensitivity. plots can called within evaluation().","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Discriminating graphs — splendid_graphs","text":"http://onlinelibrary.wiley.com/doi/10.1002/sim.5321/abstract","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Discriminating graphs — splendid_graphs","text":"Dustin Johnson, Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_graphs.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Discriminating graphs — splendid_graphs","text":"","code":"data(hgsc) class <- attr(hgsc, \"class.true\") set.seed(1) training.id <- sample(seq_along(class), replace = TRUE) test.id <- which(!seq_along(class) %in% training.id) mod <- classification(hgsc[training.id, ], class[training.id], \"xgboost\") pred <- prediction(mod, hgsc, test.id, class = class) discrimination_plot(class[test.id], attr(pred, \"prob\"))  reliability_plot(class[test.id], attr(pred, \"prob\"))  roc_plot(class[test.id], attr(pred, \"prob\")) #> Warning: All aesthetics have length 1, but the data has 2058 rows. #> ℹ Please consider using `annotate()` or provide this layer with data containing #>   a single row."},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Train, predict, and evaluate classification models — splendid_model","title":"Train, predict, and evaluate classification models — splendid_model","text":"Train, predict, evaluate classification models","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train, predict, and evaluate classification models — splendid_model","text":"","code":"splendid_model(   data,   class,   algorithms = NULL,   n = 1,   seed_boot = NULL,   seed_samp = NULL,   seed_alg = NULL,   convert = FALSE,   rfe = FALSE,   ova = FALSE,   standardize = FALSE,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   stratify = FALSE,   plus = TRUE,   threshold = 0,   trees = 100,   tune = FALSE,   vi = FALSE )"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train, predict, and evaluate classification models — splendid_model","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithms character vector algorithms use supervised learning. See Algorithms section possible options. default, argument NULL, case algorithms used. n number bootstrap replicates generate seed_boot random seed used reproducibility bootstrapping training sets model generation seed_samp random seed used reproducibility subsampling training sets model generation seed_alg random seed used reproducibility running algorithms intrinsic random element (random forests) convert logical; TRUE, converts categorical variables data dummy variables. Certain algorithms work limitations (e.g. LDA). rfe logical; TRUE, run Recursive Feature Elimination feature selection method \"lda\", \"rf\", \"svm\" algorithms. ova logical; TRUE, One-Vs-classification approach performed every algorithm algorithms. relevant results prefixed string ova_. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. stratify logical; TRUE, bootstrap resampling performed within strata class ensure bootstrap sample contains proportions strata original data. plus logical; TRUE (default), .632+ estimator calculated. Otherwise, .632 estimator calculated. threshold number 0 1 indicating lowest maximum class probability sample unclassified. trees number trees use \"rf\" tune logical; TRUE, algorithms hyperparameters tuned vi logical; TRUE, model-based variable importance scores returned algorithm available. Otherwise, SHAP-based VI scores calculated.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_model.html","id":"algorithms","dir":"Reference","previous_headings":"","what":"Algorithms","title":"Train, predict, and evaluate classification models — splendid_model","text":"classification algorithms currently supported : Prediction Analysis Microarrays (\"pam\") Support Vector Machines (\"svm\") Random Forests (\"rf\") Linear Discriminant Analysis (\"lda\") Shrinkage Linear Discriminant Analysis (\"slda\") Shrinkage Diagonal Discriminant Analysis (\"sdda\") Multinomial Logistic Regression using Generalized Linear Model penalization (\"mlr_glm\") GLM LASSO penalty (\"mlr_lasso\") GLM ridge penalty (\"mlr_ridge\") GLM elastic net penalty (\"mlr_enet\") Neural Networks (\"mlr_nnet\") Neural Networks (\"nnet\") Naive Bayes (\"nbayes\") AdaBoost.M1 (\"adaboost_m1\") Extreme Gradient Boosting (\"xgboost\") K-Nearest Neighbours (\"knn\")","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train, predict, and evaluate classification models — splendid_model","text":"","code":"data(hgsc) class <- attr(hgsc, \"class.true\") sl_result <- splendid_model(hgsc, class, n = 1, algorithms = \"xgboost\")"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":null,"dir":"Reference","previous_headings":"","what":"Process data — splendid_process","title":"Process data — splendid_process","text":"Process data converting categorical predictors dummy variables, standardizing continuous predictors, apply subsampling techniques.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Process data — splendid_process","text":"","code":"splendid_process(   data,   class,   algorithms,   convert = FALSE,   standardize = FALSE,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   seed_samp = NULL )"},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Process data — splendid_process","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning algorithms character vector algorithms use supervised learning. See Algorithms section possible options. default, argument NULL, case algorithms used. convert logical; TRUE, converts categorical variables data dummy variables. Certain algorithms work limitations (e.g. LDA). standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets. sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. seed_samp random seed used reproducibility subsampling training sets model generation","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Process data — splendid_process","text":"pre-processed data frame model training","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Process data — splendid_process","text":"variables original data already continuous, nothing done. Otherwise, conversion performed convert = TRUE using dummify(). error message thrown categorical variables convert = FALSE, indicating exactly algorithms specified require data conversion. Classification algorithms LDA MLR family limitation. Continuous predictors can scaled zero mean unit variance standardize = TRUE. Dummy variables coded 0 1 never standardized. Subsampling techniques can applied sampling methods passed subsample().","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Process data — splendid_process","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/splendid_process.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Process data — splendid_process","text":"","code":"data(hgsc) cl <- attr(hgsc, \"class.true\")  # Nothing happens if data is all continuous data_same <- splendid_process(hgsc, class = cl, algorithms = \"lda\", convert = TRUE) identical(hgsc, data_same) #> [1] FALSE  # Dummy variables created if there are categorical variables data_dummy <- splendid_process(iris, class = iris$Species, algorithms = \"lda\", convert = TRUE) head(data_dummy) #> $data #>     Sepal.Length Sepal.Width Petal.Length Petal.Width Speciesversicolor #> 1            5.1         3.5          1.4         0.2                 0 #> 2            4.9         3.0          1.4         0.2                 0 #> 3            4.7         3.2          1.3         0.2                 0 #> 4            4.6         3.1          1.5         0.2                 0 #> 5            5.0         3.6          1.4         0.2                 0 #> 6            5.4         3.9          1.7         0.4                 0 #> 7            4.6         3.4          1.4         0.3                 0 #> 8            5.0         3.4          1.5         0.2                 0 #> 9            4.4         2.9          1.4         0.2                 0 #> 10           4.9         3.1          1.5         0.1                 0 #> 11           5.4         3.7          1.5         0.2                 0 #> 12           4.8         3.4          1.6         0.2                 0 #> 13           4.8         3.0          1.4         0.1                 0 #> 14           4.3         3.0          1.1         0.1                 0 #> 15           5.8         4.0          1.2         0.2                 0 #> 16           5.7         4.4          1.5         0.4                 0 #> 17           5.4         3.9          1.3         0.4                 0 #> 18           5.1         3.5          1.4         0.3                 0 #> 19           5.7         3.8          1.7         0.3                 0 #> 20           5.1         3.8          1.5         0.3                 0 #> 21           5.4         3.4          1.7         0.2                 0 #> 22           5.1         3.7          1.5         0.4                 0 #> 23           4.6         3.6          1.0         0.2                 0 #> 24           5.1         3.3          1.7         0.5                 0 #> 25           4.8         3.4          1.9         0.2                 0 #> 26           5.0         3.0          1.6         0.2                 0 #> 27           5.0         3.4          1.6         0.4                 0 #> 28           5.2         3.5          1.5         0.2                 0 #> 29           5.2         3.4          1.4         0.2                 0 #> 30           4.7         3.2          1.6         0.2                 0 #> 31           4.8         3.1          1.6         0.2                 0 #> 32           5.4         3.4          1.5         0.4                 0 #> 33           5.2         4.1          1.5         0.1                 0 #> 34           5.5         4.2          1.4         0.2                 0 #> 35           4.9         3.1          1.5         0.2                 0 #> 36           5.0         3.2          1.2         0.2                 0 #> 37           5.5         3.5          1.3         0.2                 0 #> 38           4.9         3.6          1.4         0.1                 0 #> 39           4.4         3.0          1.3         0.2                 0 #> 40           5.1         3.4          1.5         0.2                 0 #> 41           5.0         3.5          1.3         0.3                 0 #> 42           4.5         2.3          1.3         0.3                 0 #> 43           4.4         3.2          1.3         0.2                 0 #> 44           5.0         3.5          1.6         0.6                 0 #> 45           5.1         3.8          1.9         0.4                 0 #> 46           4.8         3.0          1.4         0.3                 0 #> 47           5.1         3.8          1.6         0.2                 0 #> 48           4.6         3.2          1.4         0.2                 0 #> 49           5.3         3.7          1.5         0.2                 0 #> 50           5.0         3.3          1.4         0.2                 0 #> 51           7.0         3.2          4.7         1.4                 1 #> 52           6.4         3.2          4.5         1.5                 1 #> 53           6.9         3.1          4.9         1.5                 1 #> 54           5.5         2.3          4.0         1.3                 1 #> 55           6.5         2.8          4.6         1.5                 1 #> 56           5.7         2.8          4.5         1.3                 1 #> 57           6.3         3.3          4.7         1.6                 1 #> 58           4.9         2.4          3.3         1.0                 1 #> 59           6.6         2.9          4.6         1.3                 1 #> 60           5.2         2.7          3.9         1.4                 1 #> 61           5.0         2.0          3.5         1.0                 1 #> 62           5.9         3.0          4.2         1.5                 1 #> 63           6.0         2.2          4.0         1.0                 1 #> 64           6.1         2.9          4.7         1.4                 1 #> 65           5.6         2.9          3.6         1.3                 1 #> 66           6.7         3.1          4.4         1.4                 1 #> 67           5.6         3.0          4.5         1.5                 1 #> 68           5.8         2.7          4.1         1.0                 1 #> 69           6.2         2.2          4.5         1.5                 1 #> 70           5.6         2.5          3.9         1.1                 1 #> 71           5.9         3.2          4.8         1.8                 1 #> 72           6.1         2.8          4.0         1.3                 1 #> 73           6.3         2.5          4.9         1.5                 1 #> 74           6.1         2.8          4.7         1.2                 1 #> 75           6.4         2.9          4.3         1.3                 1 #> 76           6.6         3.0          4.4         1.4                 1 #> 77           6.8         2.8          4.8         1.4                 1 #> 78           6.7         3.0          5.0         1.7                 1 #> 79           6.0         2.9          4.5         1.5                 1 #> 80           5.7         2.6          3.5         1.0                 1 #> 81           5.5         2.4          3.8         1.1                 1 #> 82           5.5         2.4          3.7         1.0                 1 #> 83           5.8         2.7          3.9         1.2                 1 #> 84           6.0         2.7          5.1         1.6                 1 #> 85           5.4         3.0          4.5         1.5                 1 #> 86           6.0         3.4          4.5         1.6                 1 #> 87           6.7         3.1          4.7         1.5                 1 #> 88           6.3         2.3          4.4         1.3                 1 #> 89           5.6         3.0          4.1         1.3                 1 #> 90           5.5         2.5          4.0         1.3                 1 #> 91           5.5         2.6          4.4         1.2                 1 #> 92           6.1         3.0          4.6         1.4                 1 #> 93           5.8         2.6          4.0         1.2                 1 #> 94           5.0         2.3          3.3         1.0                 1 #> 95           5.6         2.7          4.2         1.3                 1 #> 96           5.7         3.0          4.2         1.2                 1 #> 97           5.7         2.9          4.2         1.3                 1 #> 98           6.2         2.9          4.3         1.3                 1 #> 99           5.1         2.5          3.0         1.1                 1 #> 100          5.7         2.8          4.1         1.3                 1 #> 101          6.3         3.3          6.0         2.5                 0 #> 102          5.8         2.7          5.1         1.9                 0 #> 103          7.1         3.0          5.9         2.1                 0 #> 104          6.3         2.9          5.6         1.8                 0 #> 105          6.5         3.0          5.8         2.2                 0 #> 106          7.6         3.0          6.6         2.1                 0 #> 107          4.9         2.5          4.5         1.7                 0 #> 108          7.3         2.9          6.3         1.8                 0 #> 109          6.7         2.5          5.8         1.8                 0 #> 110          7.2         3.6          6.1         2.5                 0 #> 111          6.5         3.2          5.1         2.0                 0 #> 112          6.4         2.7          5.3         1.9                 0 #> 113          6.8         3.0          5.5         2.1                 0 #> 114          5.7         2.5          5.0         2.0                 0 #> 115          5.8         2.8          5.1         2.4                 0 #> 116          6.4         3.2          5.3         2.3                 0 #> 117          6.5         3.0          5.5         1.8                 0 #> 118          7.7         3.8          6.7         2.2                 0 #> 119          7.7         2.6          6.9         2.3                 0 #> 120          6.0         2.2          5.0         1.5                 0 #> 121          6.9         3.2          5.7         2.3                 0 #> 122          5.6         2.8          4.9         2.0                 0 #> 123          7.7         2.8          6.7         2.0                 0 #> 124          6.3         2.7          4.9         1.8                 0 #> 125          6.7         3.3          5.7         2.1                 0 #> 126          7.2         3.2          6.0         1.8                 0 #> 127          6.2         2.8          4.8         1.8                 0 #> 128          6.1         3.0          4.9         1.8                 0 #> 129          6.4         2.8          5.6         2.1                 0 #> 130          7.2         3.0          5.8         1.6                 0 #> 131          7.4         2.8          6.1         1.9                 0 #> 132          7.9         3.8          6.4         2.0                 0 #> 133          6.4         2.8          5.6         2.2                 0 #> 134          6.3         2.8          5.1         1.5                 0 #> 135          6.1         2.6          5.6         1.4                 0 #> 136          7.7         3.0          6.1         2.3                 0 #> 137          6.3         3.4          5.6         2.4                 0 #> 138          6.4         3.1          5.5         1.8                 0 #> 139          6.0         3.0          4.8         1.8                 0 #> 140          6.9         3.1          5.4         2.1                 0 #> 141          6.7         3.1          5.6         2.4                 0 #> 142          6.9         3.1          5.1         2.3                 0 #> 143          5.8         2.7          5.1         1.9                 0 #> 144          6.8         3.2          5.9         2.3                 0 #> 145          6.7         3.3          5.7         2.5                 0 #> 146          6.7         3.0          5.2         2.3                 0 #> 147          6.3         2.5          5.0         1.9                 0 #> 148          6.5         3.0          5.2         2.0                 0 #> 149          6.2         3.4          5.4         2.3                 0 #> 150          5.9         3.0          5.1         1.8                 0 #>     Speciesvirginica #> 1                  0 #> 2                  0 #> 3                  0 #> 4                  0 #> 5                  0 #> 6                  0 #> 7                  0 #> 8                  0 #> 9                  0 #> 10                 0 #> 11                 0 #> 12                 0 #> 13                 0 #> 14                 0 #> 15                 0 #> 16                 0 #> 17                 0 #> 18                 0 #> 19                 0 #> 20                 0 #> 21                 0 #> 22                 0 #> 23                 0 #> 24                 0 #> 25                 0 #> 26                 0 #> 27                 0 #> 28                 0 #> 29                 0 #> 30                 0 #> 31                 0 #> 32                 0 #> 33                 0 #> 34                 0 #> 35                 0 #> 36                 0 #> 37                 0 #> 38                 0 #> 39                 0 #> 40                 0 #> 41                 0 #> 42                 0 #> 43                 0 #> 44                 0 #> 45                 0 #> 46                 0 #> 47                 0 #> 48                 0 #> 49                 0 #> 50                 0 #> 51                 0 #> 52                 0 #> 53                 0 #> 54                 0 #> 55                 0 #> 56                 0 #> 57                 0 #> 58                 0 #> 59                 0 #> 60                 0 #> 61                 0 #> 62                 0 #> 63                 0 #> 64                 0 #> 65                 0 #> 66                 0 #> 67                 0 #> 68                 0 #> 69                 0 #> 70                 0 #> 71                 0 #> 72                 0 #> 73                 0 #> 74                 0 #> 75                 0 #> 76                 0 #> 77                 0 #> 78                 0 #> 79                 0 #> 80                 0 #> 81                 0 #> 82                 0 #> 83                 0 #> 84                 0 #> 85                 0 #> 86                 0 #> 87                 0 #> 88                 0 #> 89                 0 #> 90                 0 #> 91                 0 #> 92                 0 #> 93                 0 #> 94                 0 #> 95                 0 #> 96                 0 #> 97                 0 #> 98                 0 #> 99                 0 #> 100                0 #> 101                1 #> 102                1 #> 103                1 #> 104                1 #> 105                1 #> 106                1 #> 107                1 #> 108                1 #> 109                1 #> 110                1 #> 111                1 #> 112                1 #> 113                1 #> 114                1 #> 115                1 #> 116                1 #> 117                1 #> 118                1 #> 119                1 #> 120                1 #> 121                1 #> 122                1 #> 123                1 #> 124                1 #> 125                1 #> 126                1 #> 127                1 #> 128                1 #> 129                1 #> 130                1 #> 131                1 #> 132                1 #> 133                1 #> 134                1 #> 135                1 #> 136                1 #> 137                1 #> 138                1 #> 139                1 #> 140                1 #> 141                1 #> 142                1 #> 143                1 #> 144                1 #> 145                1 #> 146                1 #> 147                1 #> 148                1 #> 149                1 #> 150                1 #>  #> $class #>   [1] setosa     setosa     setosa     setosa     setosa     setosa     #>   [7] setosa     setosa     setosa     setosa     setosa     setosa     #>  [13] setosa     setosa     setosa     setosa     setosa     setosa     #>  [19] setosa     setosa     setosa     setosa     setosa     setosa     #>  [25] setosa     setosa     setosa     setosa     setosa     setosa     #>  [31] setosa     setosa     setosa     setosa     setosa     setosa     #>  [37] setosa     setosa     setosa     setosa     setosa     setosa     #>  [43] setosa     setosa     setosa     setosa     setosa     setosa     #>  [49] setosa     setosa     versicolor versicolor versicolor versicolor #>  [55] versicolor versicolor versicolor versicolor versicolor versicolor #>  [61] versicolor versicolor versicolor versicolor versicolor versicolor #>  [67] versicolor versicolor versicolor versicolor versicolor versicolor #>  [73] versicolor versicolor versicolor versicolor versicolor versicolor #>  [79] versicolor versicolor versicolor versicolor versicolor versicolor #>  [85] versicolor versicolor versicolor versicolor versicolor versicolor #>  [91] versicolor versicolor versicolor versicolor versicolor versicolor #>  [97] versicolor versicolor versicolor versicolor virginica  virginica  #> [103] virginica  virginica  virginica  virginica  virginica  virginica  #> [109] virginica  virginica  virginica  virginica  virginica  virginica  #> [115] virginica  virginica  virginica  virginica  virginica  virginica  #> [121] virginica  virginica  virginica  virginica  virginica  virginica  #> [127] virginica  virginica  virginica  virginica  virginica  virginica  #> [133] virginica  virginica  virginica  virginica  virginica  virginica  #> [139] virginica  virginica  virginica  virginica  virginica  virginica  #> [145] virginica  virginica  virginica  virginica  virginica  virginica  #> Levels: setosa versicolor virginica #>   # Some algorithms are robust to the covariate data structure data_robust <- splendid_process(iris, class = iris$Species, algorithms = \"rf\", convert = FALSE) identical(iris, data_robust) #> [1] FALSE  # Standardize and down-sample iris2 <- iris[1:130, ] data_scale_down <- splendid_process(iris2, class = iris2$Species, algorithms = \"rf\", standardize = TRUE, sampling = \"down\") dim(data_scale_down) #> NULL  # Other algorithms require conversion if (FALSE) { # \\dontrun{ splendid_process(iris, class = iris$Species, algorithms = \"lda\", convert = FALSE) } # }"},{"path":"https://alinetalhouk.github.io/splendid/reference/split_data.html","id":null,"dir":"Reference","previous_headings":"","what":"Split data into training and test sets — split_data","title":"Split data into training and test sets — split_data","text":"Split data training test sets, optionally standardizing training set centers standard deviations","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/split_data.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Split data into training and test sets — split_data","text":"","code":"split_data(data, test.id = NULL, train.id = NULL, standardize = FALSE)"},{"path":"https://alinetalhouk.github.io/splendid/reference/split_data.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Split data into training and test sets — split_data","text":"data data frame rows samples, columns features test.id integer vector indices test set. NULL (default), samples used. train.id integer vector indices training set. NULL (default), samples used. standardize logical; TRUE, training sets standardized features mean zero unit variance. test sets standardized using vectors centers standard deviations used corresponding training sets.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":null,"dir":"Reference","previous_headings":"","what":"Subsampling Imbalanced Data — subsample","title":"Subsampling Imbalanced Data — subsample","text":"Subsampling imbalanced data using -sampling, -sampling, SMOTE.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Subsampling Imbalanced Data — subsample","text":"","code":"subsample(   data,   class,   sampling = c(\"none\", \"up\", \"down\", \"smote\"),   seed_samp = NULL )"},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Subsampling Imbalanced Data — subsample","text":"data data frame rows samples, columns features class true/reference class vector used supervised learning sampling default \"none\", subsampling performed. options include \"\" (-sampling minority class), \"\" (-sampling majority class), \"smote\" (synthetic points minority class -sampling majority class). Subsampling applicable training set. seed_samp random seed used reproducibility subsampling training sets model generation","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Subsampling Imbalanced Data — subsample","text":"subsampled dataset corresponding strata class balanced. resulting class variable included data output.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Subsampling Imbalanced Data — subsample","text":"deal class imbalances, can subsample data class proportions uniform.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Subsampling Imbalanced Data — subsample","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/subsample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Subsampling Imbalanced Data — subsample","text":"","code":"# Create imbalanced version of iris dataset iris_imbal <- iris[1:130, ]  # Up-sampling iris_up <- subsample(iris_imbal, iris_imbal$Species, sampling = \"up\") nrow(iris_up) #> [1] 150  # Down-sampling iris_down <- subsample(iris_imbal, iris_imbal$Species, sampling = \"down\") nrow(iris_down) #> [1] 90  # SMOTE iris_smote <- subsample(iris_imbal, iris_imbal$Species, sampling = \"smote\") nrow(iris_smote) #> [1] 101"},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":null,"dir":"Reference","previous_headings":"","what":"Variable Importance — var_imp","title":"Variable Importance — var_imp","text":"Methods calculate variable importance different classifiers.","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Variable Importance — var_imp","text":"","code":"var_imp(mod, data, ...)  # Default S3 method var_imp(mod, data, ...)  # S3 method for class 'randomForest' var_imp(mod, data, ...)  # S3 method for class 'cv.glmnet' var_imp(mod, data, ...)  # S3 method for class 'xgb.Booster' var_imp(mod, data, ...)  # S3 method for class 'nnet' var_imp(mod, data, ...)  # S3 method for class 'train' var_imp(mod, data, ...)  # S3 method for class 'svm' var_imp(mod, data, ...)"},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Variable Importance — var_imp","text":"mod model object classification() data data frame rows samples, columns features ... additional arguments passed methods","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Variable Importance — var_imp","text":"Currently, variable importance methods implemented classifiers: \"rf\" \"xgboost\", \"mlr_ridge\", \"mlr_lasso\" \"svm\" \"nnet\"","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Variable Importance — var_imp","text":"Derek Chiu","code":""},{"path":"https://alinetalhouk.github.io/splendid/reference/var_imp.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Variable Importance — var_imp","text":"","code":"data(hgsc) class <- attr(hgsc, \"class.true\") mod <- classification(hgsc, class, \"xgboost\") var_imp(mod) #> # A tibble: 91 × 2 #>    Variable Importance #>    <chr>         <dbl> #>  1 VCAN         0.119  #>  2 CXCL10       0.109  #>  3 COL5A1       0.0935 #>  4 HLA-DPA1     0.0766 #>  5 MARCKS       0.0720 #>  6 ITGB2        0.0433 #>  7 LCN2         0.0239 #>  8 STEAP3       0.0231 #>  9 STMN1        0.0220 #> 10 CLU          0.0209 #> # ℹ 81 more rows"},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-042","dir":"Changelog","previous_headings":"","what":"splendid 0.4.2","title":"splendid 0.4.2","text":"remove algorithm adaboost package maboost archived CRAN update GitHub Actions add additional methods var_imp(): cv.glmnet, svm, nnet, xgb.Booster","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-041","dir":"Changelog","previous_headings":"","what":"splendid 0.4.1","title":"splendid 0.4.1","text":"replace instances deprecated function purrr::invoke() rlang::exec() show unclassified predictions confusion matrix output exist add variable importance methods add param vi splendid() splendid_model() calculate variable importance","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-040","dir":"Changelog","previous_headings":"","what":"splendid 0.4.0","title":"splendid 0.4.0","text":"add elastic net penalty multinomial logistic regression fix calculation multi-class log loss metric increased minimum R version 4.1.0 fix warnings pertaining deprecated functions fix reordering SVM probability matrix","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-031","dir":"Changelog","previous_headings":"","what":"splendid 0.3.1","title":"splendid 0.3.1","text":"use reference cell parameterization create dummy variables factors. reference level (default first level) dummy variable. factor k levels creates k-1 dummy variables. add SVM list algorithms need numeric variables, otherwise create dummy variables","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-030","dir":"Changelog","previous_headings":"","what":"splendid 0.3.0","title":"splendid 0.3.0","text":"add GitHub Action workflows R CMD check, test coverage, pkgdown calculate AUC yardstick package DMwR archived, use SMOTE implementation package performanceEstimation print error message suggested package available fix sequential_eval() n = 1 bootstrap case","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-021","dir":"Changelog","previous_headings":"","what":"splendid 0.2.1","title":"splendid 0.2.1","text":"add Kappa evaluation metrics (overall class-specific) add class-specific accuracy G-mean metric","code":""},{"path":[]},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"new-features-0-2-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"splendid 0.2.0","text":"use “one-vs-” SMOTE subsampling technique: -sample class vs. classes combine oversampled datasets create “balanced” dataset new multiclass metric G-mean use yardstick package evaluation metrics pass seeds caret::trainControl() reproducible tuning (#48) add roc_plot() plotting multi-class ROC curves add custom print method objects returned prediction(). output previously informative long add parameter seed_samp splendid() allow setting random seed subsampling splendid_convert() now defunct. Use splendid_process() comprehensive data pre-processing step. new function can convert categorical variables dummy variables . Added ability standardize continuous variables apply sampling techniques deal class imbalance. Subsampling can occur training set. add parameter stratify allow stratified bootstrap sampling training set use standard convention confusion matrices: predicted rows, reference columns replace MLmetrics::MultiLogLoss() ModelMetrics::mlogLoss() logloss() since handles case truth category 0 counts represented probability matrix add NPV specificity evaluation()","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"minor-changes-0-2-0","dir":"Changelog","previous_headings":"","what":"Minor Changes","title":"splendid 0.2.0","text":"increase minimum R version 3.6.0 move packages used classification Suggests reduce number dependencies used conditionally remove deprecated context() tests update roxygen docs internal functions deprecated imported new packages needed update vignette parameter descriptions","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"bug-fixes-0-2-0","dir":"Changelog","previous_headings":"","what":"Bug Fixes","title":"splendid 0.2.0","text":"put macro micro averaged ROC curves end legend roc_plot() suppress warnings call multiROC::multi_roc() updates stats:::regularize.values() R-3.6.0 passes warn.collapsing = TRUE value ties stats::approx() sequential method, remove bootstrap iterations undefined F1-measure average calculation increase perc.perc.SMOTE subsampling ensure second smallest class > 0 cases decrease minsplit adaboost fewer observations needed split node rpart classifier fix num_class xgboost: number classes taken factor levels (might dropped training set) fix factor order class_threshold() take column order associated probability matrix","code":""},{"path":"https://alinetalhouk.github.io/splendid/news/index.html","id":"splendid-010","dir":"Changelog","previous_headings":"","what":"splendid 0.1.0","title":"splendid 0.1.0","text":"Default seed parameter value NULL invoke set.seed() Extend random seed parameter algorithms Extended categorical variable conversion classification() Reinstate tidy evaluation semantics package dependencies updated Improved RFE interface Added AdaBoost.M1 algorithm Added NEWS.md file track changes package.","code":""}]
