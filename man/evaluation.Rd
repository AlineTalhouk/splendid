% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/evaluation.R
\name{evaluation}
\alias{evaluation}
\title{Evaluation of prediction performance}
\usage{
evaluation(x, y, plot = FALSE)
}
\arguments{
\item{x}{true class labels}

\item{y}{predicted class labels}

\item{plot}{logical; if \code{TRUE} a discrimination plot and reliability plot are
shown for each class}
}
\value{
A list with one element per evaluation measure except for the \code{cs}
element, which returns a list of class-specific evaluation measures.
}
\description{
Evaluation of prediction performance on the OOB set is done using various
measure for classification problems.
}
\details{
The currently supported evaluation measures include discriminatory measures
like log loss, AUC, and PDI, macro-averaged PPV (Precision)/Sensitivity
(Recall)/F1-score, accuracy (same as micro-averaged PPV
Sensitivity/F1-score), Matthew's Correlation Coefficient (and its
micro-averaged analog), Kappa, G-mean, and class-specific
PPV/NPV/Sensitivity/Specificity/F1-score/MCC/Kappa/G-mean.
}
\examples{
data(hgsc)
class <- factor(attr(hgsc, "class.true"))
set.seed(1)
training.id <- sample(seq_along(class), replace = TRUE)
test.id <- which(!seq_along(class) \%in\% training.id)
mod <- classification(hgsc[training.id, ], class[training.id], "xgboost")
pred <- prediction(mod, hgsc, class, test.id)
evaluation(class[test.id], pred)
}
\author{
Derek Chiu
}
