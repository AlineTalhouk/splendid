% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classification.R
\name{classification}
\alias{classification}
\title{Multiclass classification}
\usage{
classification(
  data,
  class,
  algorithms,
  rfe = FALSE,
  ova = FALSE,
  standardize = FALSE,
  sampling = c("none", "up", "down", "smote"),
  seed_samp = NULL,
  sizes = NULL,
  trees = 100,
  tune = FALSE,
  seed_alg = NULL,
  convert = FALSE
)
}
\arguments{
\item{data}{data frame with rows as samples, columns as features}

\item{class}{true/reference class vector used for supervised learning}

\item{algorithms}{character string of algorithm to use for supervised
learning. See \strong{Algorithms} section for possible options.}

\item{rfe}{logical; if \code{TRUE}, run Recursive Feature Elimination as a feature
selection method for "lda", "rf", and "svm" algorithms.}

\item{ova}{logical; if \code{TRUE}, use the One-Vs-All approach for the \code{knn}
algorithm.}

\item{standardize}{logical; if \code{TRUE}, the training sets are standardized on
features to have mean zero and unit variance. The test sets are
standardized using the vectors of centers and standard deviations used in
corresponding training sets.}

\item{sampling}{the default is "none", in which no subsampling is performed.
Other options include "up" (Up-sampling the minority class), "down"
(Down-sampling the majority class), and "smote" (synthetic points for the
minority class and down-sampling the majority class). Subsampling is only
applicable to the training set.}

\item{seed_samp}{random seed used for reproducibility in subsampling
training sets for model generation}

\item{sizes}{the range of sizes of features to test RFE algorithm}

\item{trees}{number of trees to use in "rf"}

\item{tune}{logical; if \code{TRUE}, algorithms with hyperparameters are tuned}

\item{seed_alg}{random seed used for reproducibility when running algorithms
with an intrinsic random element (random forests)}

\item{convert}{logical; if \code{TRUE}, converts all categorical variables in
\code{data} to dummy variables. Certain algorithms only work with such
limitations (e.g. LDA).}
}
\value{
The model object from running the classification \code{algorithm}
}
\description{
Run a multiclass classification algorithm on a given dataset and reference
class.
}
\details{
Some of the classification algorithms implemented use pre-defined values that
specify settings and options while others need to tune hyperparameters.
\code{"multinom"} and \code{"nnet"} use a maximum number of weights of 2000, in case
\code{data} is high dimensional and classification is time-consuming. \code{"nnet"}
also tunes the number of nodes (1-5) in the hidden layer. \code{"pam"} considers
100 thresholds when training, and uses a uniform prior. \code{"adaboost_m1"} calls
\code{\link[adabag:boosting]{adabag::boosting()}} which supports hyperparameter tuning.

When \code{alg = "knn"}, the return value is \code{NULL} because \code{\link[class:knn]{class::knn()}} does
not output an intermediate model object. The modelling and prediction is
performed in one step. However, the class attribute "knn" is still assigned
to the result in order to call the respective \code{\link[=prediction]{prediction()}} method. An
additional class "ova" is added if \code{ova = TRUE}.
}
\section{Algorithms}{
 The classification algorithms currently supported are:
\itemize{
\item Prediction Analysis for Microarrays ("pam")
\item Support Vector Machines ("svm")
\item Random Forests ("rf")
\item Linear Discriminant Analysis ("lda")
\item Shrinkage Linear Discriminant Analysis ("slda")
\item Shrinkage Diagonal Discriminant Analysis ("sdda")
\item Multinomial Logistic Regression using
\itemize{
\item Generalized Linear Model with no penalization ("mlr_glm")
\item GLM with LASSO penalty ("mlr_lasso")
\item GLM with ridge penalty ("mlr_ridge")
\item GLM with elastic net penalty ("mlr_enet")
\item Neural Networks ("mlr_nnet")
}
\item Neural Networks ("nnet")
\item Naive Bayes ("nbayes")
\item AdaBoost.M1 ("adaboost_m1")
\item Extreme Gradient Boosting ("xgboost")
\item K-Nearest Neighbours ("knn")
}
}

\examples{
data(hgsc)
class <- attr(hgsc, "class.true")
classification(hgsc, class, "xgboost")
}
\author{
Derek Chiu
}
