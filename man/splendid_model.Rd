% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/splendid_model.R
\name{splendid_model}
\alias{splendid_model}
\title{Train, predict, and evaluate classification models}
\usage{
splendid_model(data, class, n, seed = 1, algorithms = NULL, rfe = FALSE,
  ova = FALSE, threshold = 0.5, ...)
}
\arguments{
\item{data}{data frame with rows as samples, columns as features}

\item{class}{true/reference class vector used for supervised learning}

\item{n}{number of bootstrap replicates to generate}

\item{seed}{random seed used for reproducibility in bootstrapping results}

\item{algorithms}{character vector of algorithms to use for supervised
learning. See \strong{Algorithms} section for possible options. By default,
this argument is \code{NULL}, in which case all algorithms are used.}

\item{rfe}{logical; if \code{TRUE}, run Recursive Feature Elimination as a
feature selection method for "lda", "rf", and "svm" algorithms.}

\item{ova}{logical; if \code{TRUE}, a One-Vs-All classification approach is
performed for every algorithm in \code{algorithms}. The relevant results
are prefixed with the string \code{ova_}.}

\item{threshold}{a numeric indicating the lowest maximum class probability
below which a sample will be unclassified.}

\item{...}{additional arguments to \code{splendid_model}}
}
\description{
Train, predict, and evaluate classification models
}
\section{Algorithms}{
 The classification algorithms currently supported are:
\itemize{
  \item Linear Discriminant Analysis ("lda")
  \item Shrinkage Linear Discriminant Analysis ("slda")
  \item Shrinkage Diagonal Discriminant Analysis ("sdda")
  \item Random Forests ("rf")
  \item Multinomial Classification using Neural Networks ("multinom_nnet")
  \item Neural Networks ("nnet")
  \item K-Nearest Neighbours ("knn")
  \item Support Vector Machines ("svm")
  \item Prediction Analysis for Microarrays ("pam")
  \item Adaptive Boosting ("adaboost")
  \item Extreme Gradient Boosting ("xgboost")
  \item Naive Bayes ("nbayes")
  \item Multinomial Models using the LASSO penalty ("lasso"), ridge ("ridge")
  penalty, or no regularization ("multinom_glm")
}
}

\examples{
data(hgsc)
class <- attr(hgsc, "class.true")
sl_result <- splendid_model(hgsc, class, n = 1, algorithms = "xgboost")
}
