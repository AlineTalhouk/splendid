% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/splendid_model.R
\name{splendid_model}
\alias{splendid_model}
\title{Train, predict, and evaluate classification models}
\usage{
splendid_model(
  data,
  class,
  algorithms = NULL,
  n = 1,
  seed_boot = NULL,
  seed_samp = NULL,
  seed_alg = NULL,
  convert = FALSE,
  rfe = FALSE,
  ova = FALSE,
  standardize = FALSE,
  sampling = c("none", "up", "down", "smote"),
  stratify = FALSE,
  plus = TRUE,
  threshold = 0,
  trees = 100,
  tune = FALSE,
  vi = FALSE
)
}
\arguments{
\item{data}{data frame with rows as samples, columns as features}

\item{class}{true/reference class vector used for supervised learning}

\item{algorithms}{character vector of algorithms to use for supervised
learning. See \strong{Algorithms} section for possible options. By default,
this argument is \code{NULL}, in which case all algorithms are used.}

\item{n}{number of bootstrap replicates to generate}

\item{seed_boot}{random seed used for reproducibility in bootstrapping
training sets for model generation}

\item{seed_samp}{random seed used for reproducibility in subsampling
training sets for model generation}

\item{seed_alg}{random seed used for reproducibility when running algorithms
with an intrinsic random element (random forests)}

\item{convert}{logical; if \code{TRUE}, converts all categorical variables in
\code{data} to dummy variables. Certain algorithms only work with such
limitations (e.g. LDA).}

\item{rfe}{logical; if \code{TRUE}, run Recursive Feature Elimination as a feature
selection method for "lda", "rf", and "svm" algorithms.}

\item{ova}{logical; if \code{TRUE}, a One-Vs-All classification approach is
performed for every algorithm in \code{algorithms}. The relevant results are
prefixed with the string \code{ova_}.}

\item{standardize}{logical; if \code{TRUE}, the training sets are standardized on
features to have mean zero and unit variance. The test sets are
standardized using the vectors of centers and standard deviations used in
corresponding training sets.}

\item{sampling}{the default is "none", in which no subsampling is performed.
Other options include "up" (Up-sampling the minority class), "down"
(Down-sampling the majority class), and "smote" (synthetic points for the
minority class and down-sampling the majority class). Subsampling is only
applicable to the training set.}

\item{stratify}{logical; if \code{TRUE}, the bootstrap resampling is performed
within each strata of \code{class} to ensure the bootstrap sample contains the
same proportions of each strata as the original data.}

\item{plus}{logical; if \code{TRUE} (default), the .632+ estimator is calculated.
Otherwise, the .632 estimator is calculated.}

\item{threshold}{a number between 0 and 1 indicating the lowest maximum class
probability below which a sample will be unclassified.}

\item{trees}{number of trees to use in "rf"}

\item{tune}{logical; if \code{TRUE}, algorithms with hyperparameters are tuned}

\item{vi}{logical; if \code{TRUE}, model-based variable importance scores are
returned for each algorithm if available. Otherwise, SHAP-based VI scores
are calculated.}
}
\description{
Train, predict, and evaluate classification models
}
\section{Algorithms}{
 The classification algorithms currently supported are:
\itemize{
\item Prediction Analysis for Microarrays ("pam")
\item Support Vector Machines ("svm")
\item Random Forests ("rf")
\item Linear Discriminant Analysis ("lda")
\item Shrinkage Linear Discriminant Analysis ("slda")
\item Shrinkage Diagonal Discriminant Analysis ("sdda")
\item Multinomial Logistic Regression using
\itemize{
\item Generalized Linear Model with no penalization ("mlr_glm")
\item GLM with LASSO penalty ("mlr_lasso")
\item GLM with ridge penalty ("mlr_ridge")
\item GLM with elastic net penalty ("mlr_enet")
\item Neural Networks ("mlr_nnet")
}
\item Neural Networks ("nnet")
\item Naive Bayes ("nbayes")
\item AdaBoost.M1 ("adaboost_m1")
\item Extreme Gradient Boosting ("xgboost")
\item K-Nearest Neighbours ("knn")
}
}

\examples{
data(hgsc)
class <- attr(hgsc, "class.true")
sl_result <- splendid_model(hgsc, class, n = 1, algorithms = "xgboost")
}
